{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f572a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "_=load_dotenv(find_dotenv())\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80664e",
   "metadata": {},
   "source": [
    "### Step1: Extract Data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a96838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ffdbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sayandip-saha/Desktop/CODING/Project/File'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b178ead1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sayandip-saha/Desktop/CODING/Project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f8fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Data/rag_technique.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8131d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187c389f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 0}, page_content='RAG \\n©Harshit AhluwaliaDifferent Types of\\nTechniques\\nStandard RAG Corrective RAG Speculative RAG\\nSelf RAG Agentic RAG Fusion RAG'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 1}, page_content='©Harshit AhluwaliaStandard RAG\\nStandard RAG combines a retrieval model (like a search\\nengine) with a generative model (like GPT). The retrieval\\nmodel fetches relevant documents or pieces of information\\nfrom a large database, and the generative model uses this\\ninformation to generate coherent and contextually accurate\\nresponses or content.\\n'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 2}, page_content='©Harshit AhluwaliaHow it works?\\nStep 1: Query Input - A user query or input is provided to the\\nretrieval component of the system.\\nStep 2: Retrieval Process - The retriever searches a large\\ncorpus or database for documents or text passages that are\\nmost relevant to the query. This is often done using vector\\nsearch or dense retrieval methods, where both the query and\\ndocuments are encoded into high-dimensional vectors.\\nStep 3: Selection of Top Documents - The retriever ranks the\\ndocuments based on their relevance to the query and selects\\nthe top-k documents (e.g., the top 5 most relevant passages).\\nStep 4: Generative Response - The selected documents are\\nthen passed to the generative model (like GPT). The model\\nuses this context to generate a coherent response that directly\\nanswers the query while incorporating the retrieved\\ninformation.\\nStep 5: Output - The final response is presented to the user,\\nleveraging the retrieved content to enhance accuracy and\\ndetail.'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 3}, page_content='©Harshit AhluwaliaCorrective RAG\\nStandard RAG combines a retrieval model (like a search\\nengine) with a generative model (like GPT). The retrieval\\nmodel fetches relevant documents or pieces of information\\nfrom a large database, and the generative model uses this\\ninformation to generate coherent and contextually accurate\\nresponses or content.\\n'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 4}, page_content='©Harshit AhluwaliaHow it works?\\nStep 1: Initial Retrieval and Generation - The process begins\\nlike Standard RAG, where the retriever fetches relevant\\ninformation and the generative model creates a response.\\nStep 2: Validation Process - The generated response is then\\nvalidated against a trusted dataset or source. This could\\ninvolve comparing the generated content with data from\\nauthoritative sources (like medical databases, academic\\npapers, or trusted news outlets).\\nStep 3: Correction Mechanism - If discrepancies or errors are\\ndetected during validation, the model uses the feedback to\\ncorrect the response. This might involve generating a new\\nresponse or refining the existing one.\\nStep 4: Iteration and Feedback Loop - The system iterates\\nthis process, continuously refining the response until it aligns\\nwith the correct information or falls within an acceptable error\\nmargin.\\nStep 5: Final Output - The validated and corrected response\\nis provided to the user.'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 5}, page_content='©Harshit AhluwaliaSpeculative RAG\\nSpeculative Retrieval-Augmented Generation (Speculative\\nRAG) is an approach that involves generating multiple\\npossible responses or outputs for a given input query, using a\\nretrieval model to provide relevant information. The generated\\nresponses are then evaluated using a feedback mechanism to\\nselect the most plausible or relevant one. The goal is to\\nenhance the model’s ability to produce accurate and\\ncontextually appropriate answers, especially when there is\\nambiguity or multiple potential interpretations of a query.\\n'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 6}, page_content='©Harshit AhluwaliaHow it works?\\nStep 1: Retrieval - Similar to Standard RAG, it starts by\\nretrieving multiple documents relevant to the query.\\nStep 2: Generative Speculation - The generative model\\ncreates multiple speculative responses based on the retrieved\\ndocuments. Instead of producing a single answer, it explores\\nseveral possible outputs.\\nStep 3: Feedback and Ranking - Each of the generated\\nresponses is evaluated using a feedback mechanism that\\nscores them based on various criteria like relevance,\\ncoherence, completeness, and factual accuracy. This could\\ninvolve comparing the responses against additional retrieved\\ndocuments or using scoring models.\\nStep 4: Selection Process - The model ranks all possible\\nresponses and selects the highest-scoring one as the final\\noutput.\\nStep 5: Presentation - The chosen response is then presented\\nto the user as the final answer.'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 7}, page_content='©Harshit AhluwaliaFusion RAG\\nFusion RAG integrates information from multiple retrieved\\ndocuments or sources to generate a comprehensive and well-\\nrounded response.\\n'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 8}, page_content='©Harshit AhluwaliaHow it works?\\nStep 1: Retrieval of Diverse Documents - The retriever\\nfetches multiple relevant documents, ensuring they represent\\ndiverse perspectives or cover different aspects of the query\\ntopic.\\nStep 2: Information Integration - The generative model\\nanalyzes the content of these documents and identifies\\ncommon themes, facts, or points of view. It weighs conflicting\\ninformation, integrates consistent data, and balances different\\nperspectives.\\nStep 3: Synthesis of a Unified Response - The model\\nsynthesizes a single, coherent response that integrates the\\nrelevant information from all retrieved documents. It aims to\\npresent a balanced view, combining multiple pieces of\\nevidence or viewpoints.\\nStep 4: Conflict Resolution - When conflicting information is\\npresent, the model uses additional context, source credibility,\\nor predefined rules to resolve discrepancies and form a unified\\nanswer.\\nStep 5: Output - The integrated response is provided to the\\nuser, ensuring it covers all necessary aspects of the query.'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 9}, page_content='©Harshit AhluwaliaAgentic RAG\\nAgentic RAG involves the AI acting autonomously with a\\npredefined goal, using the retrieval process to make decisions\\nand guide its actions.\\n'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 10}, page_content='©Harshit AhluwaliaHow it works?\\nStep 1: Goal Definition - The model is provided with a specific\\nobjective or goal (e.g., explaining a concept, providing\\npersonalized tutoring, or solving a complex task).\\nStep 2: Autonomous Action Planning - The generative model\\nautonomously determines which information to retrieve and\\nwhich actions to take to achieve the goal. It may ask itself\\nquestions, plan steps, or decide on intermediate objectives.\\nStep 3: Iterative Retrieval and Generation - The model\\nretrieves information in iterative cycles, each time refining its\\nunderstanding of the task or goal. It may adapt its retrieval\\nstrategy based on new insights gained from previous\\niterations.\\nStep 4: Dynamic Adjustment - The model continuously\\nevaluates its progress toward the goal, adjusting its actions,\\nretrieval strategies, or questions dynamically based on\\nfeedback or partial results.\\nStep 5: Completion of Goal - Once the model determines that\\nit has achieved the goal or completed the task, it generates a\\nfinal output that reflects its findings or solution.'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 11}, page_content='©Harshit AhluwaliaSelf RAG\\nSelf RAG uses the model’s own generated outputs as new\\ndata for future retrieval, enabling continuous self-improvement\\nand learning over time.\\n'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 12}, page_content='©Harshit AhluwaliaHow it works?\\nStep 1: Initial Retrieval and Generation - The model begins by\\nretrieving information and generating responses based on the\\ninput query.\\nStep 2: Self-Storage - The generated outputs are stored in a\\ndedicated repository as new data points.\\nStep 3: Iterative Self-Retrieval - For future queries, the model\\nretrieves not only from the original corpus but also from its\\nown previously generated content. This allows it to build upon\\nits past outputs, refining them further.\\nStep 4: Self-Improvement Loop - As more content is\\ngenerated and stored, the model learns from its past\\nresponses, identifying patterns, gaps, or inaccuracies, and\\ncontinuously adjusting its retrieval and generation strategies to\\nimprove over time.\\nStep 5: Enhanced Output - Over successive iterations, the\\nmodel produces increasingly accurate and context-aware\\nresponses, leveraging the self-referential data it has\\naccumulated.'),\n",
       " Document(metadata={'source': 'Data/rag_technique.pdf', 'page': 13}, page_content='Moreover, \\nwe are offering a \\n@Harshit AhluwaliaFree Certification \\non RAGs, check the link\\nin the description')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "330a059b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d206818",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_data = \"\"\n",
    "for page in data:\n",
    "    entire_data += page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216fcf18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG \\n©Harshit AhluwaliaDifferent Types of\\nTechniques\\nStandard RAG Corrective RAG Speculative RAG\\nSelf RAG Agentic RAG Fusion RAG©Harshit AhluwaliaStandard RAG\\nStandard RAG combines a retrieval model (like a search\\nengine) with a generative model (like GPT). The retrieval\\nmodel fetches relevant documents or pieces of information\\nfrom a large database, and the generative model uses this\\ninformation to generate coherent and contextually accurate\\nresponses or content.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Query Input - A user query or input is provided to the\\nretrieval component of the system.\\nStep 2: Retrieval Process - The retriever searches a large\\ncorpus or database for documents or text passages that are\\nmost relevant to the query. This is often done using vector\\nsearch or dense retrieval methods, where both the query and\\ndocuments are encoded into high-dimensional vectors.\\nStep 3: Selection of Top Documents - The retriever ranks the\\ndocuments based on their relevance to the query and selects\\nthe top-k documents (e.g., the top 5 most relevant passages).\\nStep 4: Generative Response - The selected documents are\\nthen passed to the generative model (like GPT). The model\\nuses this context to generate a coherent response that directly\\nanswers the query while incorporating the retrieved\\ninformation.\\nStep 5: Output - The final response is presented to the user,\\nleveraging the retrieved content to enhance accuracy and\\ndetail.©Harshit AhluwaliaCorrective RAG\\nStandard RAG combines a retrieval model (like a search\\nengine) with a generative model (like GPT). The retrieval\\nmodel fetches relevant documents or pieces of information\\nfrom a large database, and the generative model uses this\\ninformation to generate coherent and contextually accurate\\nresponses or content.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Initial Retrieval and Generation - The process begins\\nlike Standard RAG, where the retriever fetches relevant\\ninformation and the generative model creates a response.\\nStep 2: Validation Process - The generated response is then\\nvalidated against a trusted dataset or source. This could\\ninvolve comparing the generated content with data from\\nauthoritative sources (like medical databases, academic\\npapers, or trusted news outlets).\\nStep 3: Correction Mechanism - If discrepancies or errors are\\ndetected during validation, the model uses the feedback to\\ncorrect the response. This might involve generating a new\\nresponse or refining the existing one.\\nStep 4: Iteration and Feedback Loop - The system iterates\\nthis process, continuously refining the response until it aligns\\nwith the correct information or falls within an acceptable error\\nmargin.\\nStep 5: Final Output - The validated and corrected response\\nis provided to the user.©Harshit AhluwaliaSpeculative RAG\\nSpeculative Retrieval-Augmented Generation (Speculative\\nRAG) is an approach that involves generating multiple\\npossible responses or outputs for a given input query, using a\\nretrieval model to provide relevant information. The generated\\nresponses are then evaluated using a feedback mechanism to\\nselect the most plausible or relevant one. The goal is to\\nenhance the model’s ability to produce accurate and\\ncontextually appropriate answers, especially when there is\\nambiguity or multiple potential interpretations of a query.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Retrieval - Similar to Standard RAG, it starts by\\nretrieving multiple documents relevant to the query.\\nStep 2: Generative Speculation - The generative model\\ncreates multiple speculative responses based on the retrieved\\ndocuments. Instead of producing a single answer, it explores\\nseveral possible outputs.\\nStep 3: Feedback and Ranking - Each of the generated\\nresponses is evaluated using a feedback mechanism that\\nscores them based on various criteria like relevance,\\ncoherence, completeness, and factual accuracy. This could\\ninvolve comparing the responses against additional retrieved\\ndocuments or using scoring models.\\nStep 4: Selection Process - The model ranks all possible\\nresponses and selects the highest-scoring one as the final\\noutput.\\nStep 5: Presentation - The chosen response is then presented\\nto the user as the final answer.©Harshit AhluwaliaFusion RAG\\nFusion RAG integrates information from multiple retrieved\\ndocuments or sources to generate a comprehensive and well-\\nrounded response.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Retrieval of Diverse Documents - The retriever\\nfetches multiple relevant documents, ensuring they represent\\ndiverse perspectives or cover different aspects of the query\\ntopic.\\nStep 2: Information Integration - The generative model\\nanalyzes the content of these documents and identifies\\ncommon themes, facts, or points of view. It weighs conflicting\\ninformation, integrates consistent data, and balances different\\nperspectives.\\nStep 3: Synthesis of a Unified Response - The model\\nsynthesizes a single, coherent response that integrates the\\nrelevant information from all retrieved documents. It aims to\\npresent a balanced view, combining multiple pieces of\\nevidence or viewpoints.\\nStep 4: Conflict Resolution - When conflicting information is\\npresent, the model uses additional context, source credibility,\\nor predefined rules to resolve discrepancies and form a unified\\nanswer.\\nStep 5: Output - The integrated response is provided to the\\nuser, ensuring it covers all necessary aspects of the query.©Harshit AhluwaliaAgentic RAG\\nAgentic RAG involves the AI acting autonomously with a\\npredefined goal, using the retrieval process to make decisions\\nand guide its actions.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Goal Definition - The model is provided with a specific\\nobjective or goal (e.g., explaining a concept, providing\\npersonalized tutoring, or solving a complex task).\\nStep 2: Autonomous Action Planning - The generative model\\nautonomously determines which information to retrieve and\\nwhich actions to take to achieve the goal. It may ask itself\\nquestions, plan steps, or decide on intermediate objectives.\\nStep 3: Iterative Retrieval and Generation - The model\\nretrieves information in iterative cycles, each time refining its\\nunderstanding of the task or goal. It may adapt its retrieval\\nstrategy based on new insights gained from previous\\niterations.\\nStep 4: Dynamic Adjustment - The model continuously\\nevaluates its progress toward the goal, adjusting its actions,\\nretrieval strategies, or questions dynamically based on\\nfeedback or partial results.\\nStep 5: Completion of Goal - Once the model determines that\\nit has achieved the goal or completed the task, it generates a\\nfinal output that reflects its findings or solution.©Harshit AhluwaliaSelf RAG\\nSelf RAG uses the model’s own generated outputs as new\\ndata for future retrieval, enabling continuous self-improvement\\nand learning over time.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Initial Retrieval and Generation - The model begins by\\nretrieving information and generating responses based on the\\ninput query.\\nStep 2: Self-Storage - The generated outputs are stored in a\\ndedicated repository as new data points.\\nStep 3: Iterative Self-Retrieval - For future queries, the model\\nretrieves not only from the original corpus but also from its\\nown previously generated content. This allows it to build upon\\nits past outputs, refining them further.\\nStep 4: Self-Improvement Loop - As more content is\\ngenerated and stored, the model learns from its past\\nresponses, identifying patterns, gaps, or inaccuracies, and\\ncontinuously adjusting its retrieval and generation strategies to\\nimprove over time.\\nStep 5: Enhanced Output - Over successive iterations, the\\nmodel produces increasingly accurate and context-aware\\nresponses, leveraging the self-referential data it has\\naccumulated.Moreover, \\nwe are offering a \\n@Harshit AhluwaliaFree Certification \\non RAGs, check the link\\nin the description'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641feaf5",
   "metadata": {},
   "source": [
    "### Step2: Get chunks out of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faebd889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09428c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter = TokenTextSplitter(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f91bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2117255",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_question_gen = spliter.split_text(entire_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5f0763b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunk_question_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20943efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_question_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ed62b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunk_question_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5285ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ques_gen = [Document(page_content=i) for i in chunk_question_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66bdb3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document_ques_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "324c55e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c98e039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ans_gen = spliter.split_documents(document_ques_gen) # split operation on document (list of Document datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea2c2c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_ans_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff949196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='RAG \\n©Harshit AhluwaliaDifferent Types of\\nTechniques\\nStandard RAG Corrective RAG Speculative RAG\\nSelf RAG Agentic RAG Fusion RAG©Harshit AhluwaliaStandard RAG\\nStandard RAG combines a retrieval model (like a search\\nengine) with a generative model (like GPT). The retrieval\\nmodel fetches relevant documents or pieces of information\\nfrom a large database, and the generative model uses this\\ninformation to generate coherent and contextually accurate\\nresponses or content.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Query Input - A user query or input is provided to the\\nretrieval component of the system.\\nStep 2: Retrieval Process - The retriever searches a large\\ncorpus or database for documents or text passages that are\\nmost relevant to the query. This is often done using vector\\nsearch or dense retrieval methods, where both the query and\\ndocuments are encoded into high-dimensional vectors.\\nStep 3: Selection of Top Documents - The retriever ranks the\\ndocuments based on their relevance to the query and selects\\nthe top-k documents (e.g., the top 5 most relevant passages).\\nStep 4: Generative Response - The selected documents are\\nthen passed to the generative model (like GPT). The model\\nuses this context to generate a coherent response that directly\\nanswers the query while incorporating the retrieved\\ninformation.\\nStep 5: Output - The final response is presented to the user,\\nleveraging the retrieved content to enhance accuracy and\\ndetail.©Harshit AhluwaliaCorrective RAG\\nStandard RAG combines a retrieval model (like a search\\nengine) with a generative model (like GPT). The retrieval\\nmodel fetches relevant documents or pieces of information\\nfrom a large database, and the generative model uses this\\ninformation to generate coherent and contextually accurate\\nresponses or content.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Initial Retrieval and Generation - The process begins\\nlike Standard RAG, where the retriever fetches relevant\\ninformation and the generative model creates a response.\\nStep 2: Validation Process - The generated response is then\\nvalidated against a trusted dataset or source. This could\\ninvolve comparing the generated content with data from\\nauthoritative sources (like medical databases, academic\\npapers, or trusted news outlets).\\nStep 3: Correction Mechanism - If discrepancies or errors are\\ndetected during validation, the model uses the feedback to\\ncorrect the response. This might involve generating a new\\nresponse or refining the existing one.\\nStep 4: Iteration and Feedback Loop - The system iterates\\nthis process, continuously refining the response until it aligns\\nwith the correct information or falls within an acceptable error\\nmargin.\\nStep 5: Final Output - The validated and corrected response\\nis provided to the user.©Harshit AhluwaliaSpeculative RAG\\nSpeculative Retrieval-Augmented Generation (Speculative\\nRAG) is an approach that involves generating multiple\\npossible responses or outputs for a given input query, using a\\nretrieval model to provide relevant information. The generated\\nresponses are then evaluated using a feedback mechanism to\\nselect the most plausible or relevant one. The goal is to\\nenhance the model’s ability to produce accurate and\\ncontextually appropriate answers, especially when there is\\nambiguity or multiple potential interpretations of a query.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Retrieval - Similar to Standard RAG, it starts by\\nretrieving multiple documents relevant to the query.\\nStep 2: Generative Speculation - The generative model\\ncreates multiple speculative responses based on the retrieved\\ndocuments. Instead of producing a single answer, it explores\\nseveral possible outputs.\\nStep 3: Feedback and Ranking - Each of the generated\\nresponses is evaluated using a feedback mechanism that\\nscores them based on various criteria like relevance,\\ncoherence, completeness, and factual accuracy. This could\\ninvolve comparing the responses against additional retrieved\\ndocuments or using scoring models.\\nStep 4: Selection Process - The model ranks all possible\\nresponses and selects the highest-scoring one as the final\\noutput.\\nStep 5: Presentation - The chosen response is then presented\\nto the user as the final answer.©Harshit AhluwaliaFusion RAG\\nFusion RAG integrates information from multiple retrieved\\ndocuments or sources to generate a comprehensive and well-\\nrounded response.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Retrieval of Diverse Documents - The retriever\\nfetches multiple relevant documents, ensuring they represent\\ndiverse perspectives or cover different aspects of the query\\ntopic.\\nStep 2: Information Integration - The generative model\\nanalyzes the content of these documents and identifies\\ncommon themes, facts, or points of view. It weighs conflicting\\ninformation, integrates consistent data, and balances different\\nperspectives.\\nStep 3: Synthesis of a Unified Response - The model\\nsynthesizes a single, coherent response that integrates the\\nrelevant information from all retrieved documents. It aims to\\npresent a balanced view, combining multiple pieces of\\nevidence or viewpoints.\\nStep 4: Conflict Resolution - When conflicting information is\\npresent, the model uses additional context, source credibility,\\nor predefined rules to resolve discrepancies and form a unified\\nanswer.\\nStep 5: Output - The integrated response is provided to the\\nuser, ensuring it covers all necessary aspects of the query.©Harshit AhluwaliaAgentic RAG\\nAgentic RAG involves the AI acting autonomously with a\\npredefined goal, using the retrieval process to make decisions\\nand guide its actions.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Goal Definition - The model is provided with a specific\\nobjective or goal (e.g., explaining a concept, providing\\npersonalized tutoring, or solving a complex task).\\nStep 2: Autonomous Action Planning - The generative model\\nautonomously determines which information to retrieve and\\nwhich actions to take to achieve the goal. It may ask itself\\nquestions, plan steps, or decide on intermediate objectives.\\nStep 3: Iterative Retrieval and Generation - The model\\nretrieves information in iterative cycles, each time refining its\\nunderstanding of the task or goal. It may adapt its retrieval\\nstrategy based on new insights gained from previous\\niterations.\\nStep 4: Dynamic Adjustment - The model continuously\\nevaluates its progress toward the goal, adjusting its actions,\\nretrieval strategies, or questions dynamically based on\\nfeedback or partial results.\\nStep 5: Completion of Goal - Once the model determines that\\nit has achieved the goal or completed the task, it generates a\\nfinal output that reflects its findings or solution.©Harshit AhluwaliaSelf RAG\\nSelf RAG uses the model’s own generated outputs as new\\ndata for future retrieval, enabling continuous self-improvement\\nand learning over time.\\n©Harshit AhluwaliaHow it works?\\nStep 1: Initial Retrieval and Generation - The model begins by\\nretrieving information and generating responses based on the\\ninput query.\\nStep 2: Self-Storage - The generated outputs are stored in a\\ndedicated repository as new data points.\\nStep 3: Iterative Self-Retrieval - For future queries, the model\\nretrieves not only from the original corpus but also from its\\nown previously generated content. This allows it to build upon\\nits past outputs, refining them further.\\nStep 4: Self-Improvement Loop - As more content is\\ngenerated and stored, the model learns from its past\\nresponses, identifying patterns, gaps, or inaccuracies, and\\ncontinuously adjusting its retrieval and generation strategies to\\nimprove over time.\\nStep 5: Enhanced Output - Over successive iterations, the\\nmodel produces increasingly accurate and context-aware\\nresponses, leveraging the self-referential data it has\\naccumulated.Moreover, \\nwe are offering a \\n@Harshit AhluwaliaFree Certification \\non RAGs, check the link\\nin the description')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_ans_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b79b7",
   "metadata": {},
   "source": [
    "### Step3: Load LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09bd98e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=1.7, # Criteria for creativity\n",
    "    max_tokens=4096 # max new tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ffb1d",
   "metadata": {},
   "source": [
    "### Step4: Define prompt template and generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a214bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    You are an expert at creating questions based on moden GEN AI materials and Documentation.\n",
    "    Your goal is to prepare user for their interviews regarding AI Engineering roles.\n",
    "    You do this by asking questions about the document below\n",
    "    \n",
    "    {text}\n",
    "\n",
    "    Create uestion out of the document provided to you. Make sure you don't loose any information regarding the document.\n",
    "\n",
    "    Questions: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb20826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00903d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt_questions = PromptTemplate(template=prompt_template,input_variables=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5fabc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_template = (\"\"\"\n",
    "    You are an expert at creating questions based on moden GEN AI materials and Documentation.\n",
    "    Your goal is to prepare user for their interviews regarding AI Engineering roles.\n",
    "    we have recieved some practice questions to each certain extent : {existing_answer}.\n",
    "    we have the option to refine the existing question orr add new ones.\n",
    "    (only if necessary) with some more context below.\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Given the new context, refine the original question in english. If the context is not useful please provide the original question.\n",
    "    Question: \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18b6043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_prompt_question = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\",\"text\"],\n",
    "    template=refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6c169c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eadf96d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question_generation_chain = load_summarize_chain(\n",
    "    llm=model,\n",
    "    chain_type=\"refine\",\n",
    "    verbose=True,\n",
    "    question_prompt=Prompt_questions,\n",
    "    refine_prompt = refine_prompt_question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7f8fe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4900/3100231773.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  Ques = Question_generation_chain.run(document_ques_gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert at creating questions based on moden GEN AI materials and Documentation.\n",
      "    Your goal is to prepare user for their interviews regarding AI Engineering roles.\n",
      "    You do this by asking questions about the document below\n",
      "\n",
      "    RAG \n",
      "©Harshit AhluwaliaDifferent Types of\n",
      "Techniques\n",
      "Standard RAG Corrective RAG Speculative RAG\n",
      "Self RAG Agentic RAG Fusion RAG©Harshit AhluwaliaStandard RAG\n",
      "Standard RAG combines a retrieval model (like a search\n",
      "engine) with a generative model (like GPT). The retrieval\n",
      "model fetches relevant documents or pieces of information\n",
      "from a large database, and the generative model uses this\n",
      "information to generate coherent and contextually accurate\n",
      "responses or content.\n",
      "©Harshit AhluwaliaHow it works?\n",
      "Step 1: Query Input - A user query or input is provided to the\n",
      "retrieval component of the system.\n",
      "Step 2: Retrieval Process - The retriever searches a large\n",
      "corpus or database for documents or text passages that are\n",
      "most relevant to the query. This is often done using vector\n",
      "search or dense retrieval methods, where both the query and\n",
      "documents are encoded into high-dimensional vectors.\n",
      "Step 3: Selection of Top Documents - The retriever ranks the\n",
      "documents based on their relevance to the query and selects\n",
      "the top-k documents (e.g., the top 5 most relevant passages).\n",
      "Step 4: Generative Response - The selected documents are\n",
      "then passed to the generative model (like GPT). The model\n",
      "uses this context to generate a coherent response that directly\n",
      "answers the query while incorporating the retrieved\n",
      "information.\n",
      "Step 5: Output - The final response is presented to the user,\n",
      "leveraging the retrieved content to enhance accuracy and\n",
      "detail.©Harshit AhluwaliaCorrective RAG\n",
      "Standard RAG combines a retrieval model (like a search\n",
      "engine) with a generative model (like GPT). The retrieval\n",
      "model fetches relevant documents or pieces of information\n",
      "from a large database, and the generative model uses this\n",
      "information to generate coherent and contextually accurate\n",
      "responses or content.\n",
      "©Harshit AhluwaliaHow it works?\n",
      "Step 1: Initial Retrieval and Generation - The process begins\n",
      "like Standard RAG, where the retriever fetches relevant\n",
      "information and the generative model creates a response.\n",
      "Step 2: Validation Process - The generated response is then\n",
      "validated against a trusted dataset or source. This could\n",
      "involve comparing the generated content with data from\n",
      "authoritative sources (like medical databases, academic\n",
      "papers, or trusted news outlets).\n",
      "Step 3: Correction Mechanism - If discrepancies or errors are\n",
      "detected during validation, the model uses the feedback to\n",
      "correct the response. This might involve generating a new\n",
      "response or refining the existing one.\n",
      "Step 4: Iteration and Feedback Loop - The system iterates\n",
      "this process, continuously refining the response until it aligns\n",
      "with the correct information or falls within an acceptable error\n",
      "margin.\n",
      "Step 5: Final Output - The validated and corrected response\n",
      "is provided to the user.©Harshit AhluwaliaSpeculative RAG\n",
      "Speculative Retrieval-Augmented Generation (Speculative\n",
      "RAG) is an approach that involves generating multiple\n",
      "possible responses or outputs for a given input query, using a\n",
      "retrieval model to provide relevant information. The generated\n",
      "responses are then evaluated using a feedback mechanism to\n",
      "select the most plausible or relevant one. The goal is to\n",
      "enhance the model’s ability to produce accurate and\n",
      "contextually appropriate answers, especially when there is\n",
      "ambiguity or multiple potential interpretations of a query.\n",
      "©Harshit AhluwaliaHow it works?\n",
      "Step 1: Retrieval - Similar to Standard RAG, it starts by\n",
      "retrieving multiple documents relevant to the query.\n",
      "Step 2: Generative Speculation - The generative model\n",
      "creates multiple speculative responses based on the retrieved\n",
      "documents. Instead of producing a single answer, it explores\n",
      "several possible outputs.\n",
      "Step 3: Feedback and Ranking - Each of the generated\n",
      "responses is evaluated using a feedback mechanism that\n",
      "scores them based on various criteria like relevance,\n",
      "coherence, completeness, and factual accuracy. This could\n",
      "involve comparing the responses against additional retrieved\n",
      "documents or using scoring models.\n",
      "Step 4: Selection Process - The model ranks all possible\n",
      "responses and selects the highest-scoring one as the final\n",
      "output.\n",
      "Step 5: Presentation - The chosen response is then presented\n",
      "to the user as the final answer.©Harshit AhluwaliaFusion RAG\n",
      "Fusion RAG integrates information from multiple retrieved\n",
      "documents or sources to generate a comprehensive and well-\n",
      "rounded response.\n",
      "©Harshit AhluwaliaHow it works?\n",
      "Step 1: Retrieval of Diverse Documents - The retriever\n",
      "fetches multiple relevant documents, ensuring they represent\n",
      "diverse perspectives or cover different aspects of the query\n",
      "topic.\n",
      "Step 2: Information Integration - The generative model\n",
      "analyzes the content of these documents and identifies\n",
      "common themes, facts, or points of view. It weighs conflicting\n",
      "information, integrates consistent data, and balances different\n",
      "perspectives.\n",
      "Step 3: Synthesis of a Unified Response - The model\n",
      "synthesizes a single, coherent response that integrates the\n",
      "relevant information from all retrieved documents. It aims to\n",
      "present a balanced view, combining multiple pieces of\n",
      "evidence or viewpoints.\n",
      "Step 4: Conflict Resolution - When conflicting information is\n",
      "present, the model uses additional context, source credibility,\n",
      "or predefined rules to resolve discrepancies and form a unified\n",
      "answer.\n",
      "Step 5: Output - The integrated response is provided to the\n",
      "user, ensuring it covers all necessary aspects of the query.©Harshit AhluwaliaAgentic RAG\n",
      "Agentic RAG involves the AI acting autonomously with a\n",
      "predefined goal, using the retrieval process to make decisions\n",
      "and guide its actions.\n",
      "©Harshit AhluwaliaHow it works?\n",
      "Step 1: Goal Definition - The model is provided with a specific\n",
      "objective or goal (e.g., explaining a concept, providing\n",
      "personalized tutoring, or solving a complex task).\n",
      "Step 2: Autonomous Action Planning - The generative model\n",
      "autonomously determines which information to retrieve and\n",
      "which actions to take to achieve the goal. It may ask itself\n",
      "questions, plan steps, or decide on intermediate objectives.\n",
      "Step 3: Iterative Retrieval and Generation - The model\n",
      "retrieves information in iterative cycles, each time refining its\n",
      "understanding of the task or goal. It may adapt its retrieval\n",
      "strategy based on new insights gained from previous\n",
      "iterations.\n",
      "Step 4: Dynamic Adjustment - The model continuously\n",
      "evaluates its progress toward the goal, adjusting its actions,\n",
      "retrieval strategies, or questions dynamically based on\n",
      "feedback or partial results.\n",
      "Step 5: Completion of Goal - Once the model determines that\n",
      "it has achieved the goal or completed the task, it generates a\n",
      "final output that reflects its findings or solution.©Harshit AhluwaliaSelf RAG\n",
      "Self RAG uses the model’s own generated outputs as new\n",
      "data for future retrieval, enabling continuous self-improvement\n",
      "and learning over time.\n",
      "©Harshit AhluwaliaHow it works?\n",
      "Step 1: Initial Retrieval and Generation - The model begins by\n",
      "retrieving information and generating responses based on the\n",
      "input query.\n",
      "Step 2: Self-Storage - The generated outputs are stored in a\n",
      "dedicated repository as new data points.\n",
      "Step 3: Iterative Self-Retrieval - For future queries, the model\n",
      "retrieves not only from the original corpus but also from its\n",
      "own previously generated content. This allows it to build upon\n",
      "its past outputs, refining them further.\n",
      "Step 4: Self-Improvement Loop - As more content is\n",
      "generated and stored, the model learns from its past\n",
      "responses, identifying patterns, gaps, or inaccuracies, and\n",
      "continuously adjusting its retrieval and generation strategies to\n",
      "improve over time.\n",
      "Step 5: Enhanced Output - Over successive iterations, the\n",
      "model produces increasingly accurate and context-aware\n",
      "responses, leveraging the self-referential data it has\n",
      "accumulated.Moreover, \n",
      "we are offering a \n",
      "@Harshit AhluwaliaFree Certification \n",
      "on RAGs, check the link\n",
      "in the description\n",
      "\n",
      "    Create uestion out of the document provided to you. Make sure you don't loose any information regarding the document.\n",
      "\n",
      "    Questions: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Ques = Question_generation_chain.run(document_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e868edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Questions – RAG (Retrieval‑Augmented Generation) Primer**\n",
      "\n",
      "1. **What is Standard RAG and what are the two core components it combines?**  \n",
      "   a) Explain how the retrieval model and generative model cooperate.  \n",
      "   b) Provide an example of a use‑case where this architecture shines.\n",
      "\n",
      "2. **Walk through each step of the Standard RAG workflow (from query input to final output).**  \n",
      "   – For every step, name the typical methods used (e.g., vector search, dense retrieval).\n",
      "\n",
      "3. **Explain the objective of a Corrective RAG system and the main difference between it and Standard RAG.**  \n",
      "   – Describe the validation step: what kinds of sources could serve as “trusted datasets”?  \n",
      "   – How does the correction mechanism refine or regenerate a response?\n",
      "\n",
      "4. **Speculative RAG attempts to handle ambiguity or multiple interpretations.**  \n",
      "   – List the steps taken from retrieval to the final selection of a response.  \n",
      "   – How is the feedback‑based ranking of the speculative answers usually implemented?\n",
      "\n",
      "5. **What distinguishes Fusion RAG from the previous variants?**  \n",
      "   – Detail how the system deals with **conflicting information** in the retrieved documents.  \n",
      "   – Why is integrating “diverse perspectives” valuable for an end‑user?\n",
      "\n",
      "6. **Agentic RAG incorporates autonomous action planning.**  \n",
      "   – How does it differ from purely reactive RAG approaches?  \n",
      "   – In what ways can it iterate on retrieval strategies throughout a task?\n",
      "\n",
      "7. **Describe the Self‑RAG loop.**  \n",
      "   – After generation, what is stored for future retrieval and why is that beneficial?  \n",
      "   – Illustrate a scenario where self‑retrieval improves the accuracy of subsequent responses.\n",
      "\n",
      "8. **For each RAG variant, identify one challenge or limitation that the architecture must overcome.**\n",
      "\n",
      "9. **Why might an organization opt for a Speculative or Fusion RAG variant rather than Standard RAG when working on a policy‑driven or regulatory compliance chatbot?**\n",
      "\n",
      "10. **In the context of a “Free Certification on RAGs” mentioned at the end of the document, what specific competencies or learning outcomes would you expect participants to attain?**  \n",
      "    – Which RAG techniques would most likely be emphasized?\n",
      "\n",
      "*(These questions cover every element presented in the source – the definitions, step‑by‑step procedures, comparative benefits, practical scenarios, and the certification mention—ensuring none of the provided information is omitted.)*\n"
     ]
    }
   ],
   "source": [
    "print(Ques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69acf74a",
   "metadata": {},
   "source": [
    "### Step5 : Feed the questions and get the corresponding answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "389ba2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4900/2795403779.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "/home/sayandip-saha/Desktop/CODING/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-25 15:03:01.594972: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-25 15:03:02.766951: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-25 15:03:03.821503: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756114384.974876    4900 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756114385.014573    4900 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756114385.276380    4900 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756114385.276418    4900 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756114385.276421    4900 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756114385.276423    4900 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-25 15:03:05.308702: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f18a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "710d3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(document_ques_gen,embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6aeaaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_list = Ques.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39002d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['**Questions – RAG (Retrieval‑Augmented Generation) Primer**', '', '1. **What is Standard RAG and what are the two core components it combines?**  ', '   a) Explain how the retrieval model and generative model cooperate.  ', '   b) Provide an example of a use‑case where this architecture shines.', '', '2. **Walk through each step of the Standard RAG workflow (from query input to final output).**  ', '   – For every step, name the typical methods used (e.g., vector search, dense retrieval).', '', '3. **Explain the objective of a Corrective RAG system and the main difference between it and Standard RAG.**  ', '   – Describe the validation step: what kinds of sources could serve as “trusted datasets”?  ', '   – How does the correction mechanism refine or regenerate a response?', '', '4. **Speculative RAG attempts to handle ambiguity or multiple interpretations.**  ', '   – List the steps taken from retrieval to the final selection of a response.  ', '   – How is the feedback‑based ranking of the speculative answers usually implemented?', '', '5. **What distinguishes Fusion RAG from the previous variants?**  ', '   – Detail how the system deals with **conflicting information** in the retrieved documents.  ', '   – Why is integrating “diverse perspectives” valuable for an end‑user?', '', '6. **Agentic RAG incorporates autonomous action planning.**  ', '   – How does it differ from purely reactive RAG approaches?  ', '   – In what ways can it iterate on retrieval strategies throughout a task?', '', '7. **Describe the Self‑RAG loop.**  ', '   – After generation, what is stored for future retrieval and why is that beneficial?  ', '   – Illustrate a scenario where self‑retrieval improves the accuracy of subsequent responses.', '', '8. **For each RAG variant, identify one challenge or limitation that the architecture must overcome.**', '', '9. **Why might an organization opt for a Speculative or Fusion RAG variant rather than Standard RAG when working on a policy‑driven or regulatory compliance chatbot?**', '', '10. **In the context of a “Free Certification on RAGs” mentioned at the end of the document, what specific competencies or learning outcomes would you expect participants to attain?**  ', '    – Which RAG techniques would most likely be emphasized?', '', '*(These questions cover every element presented in the source – the definitions, step‑by‑step procedures, comparative benefits, practical scenarios, and the certification mention—ensuring none of the provided information is omitted.)*']\n"
     ]
    }
   ],
   "source": [
    "print(ques_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d24427d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Questions – RAG (Retrieval‑Augmented Generation) Primer**\\n\\n1. **What is Standard RAG and what are the two core components it combines?**  \\n   a) Explain how the retrieval model and generative model cooperate.  \\n   b) Provide an example of a use‑case where this architecture shines.\\n\\n2. **Walk through each step of the Standard RAG workflow (from query input to final output).**  \\n   – For every step, name the typical methods used (e.g., vector search, dense retrieval).\\n\\n3. **Explain the objective of a Corrective RAG system and the main difference between it and Standard RAG.**  \\n   – Describe the validation step: what kinds of sources could serve as “trusted datasets”?  \\n   – How does the correction mechanism refine or regenerate a response?\\n\\n4. **Speculative RAG attempts to handle ambiguity or multiple interpretations.**  \\n   – List the steps taken from retrieval to the final selection of a response.  \\n   – How is the feedback‑based ranking of the speculative answers usually implemented?\\n\\n5. **What distinguishes Fusion RAG from the previous variants?**  \\n   – Detail how the system deals with **conflicting information** in the retrieved documents.  \\n   – Why is integrating “diverse perspectives” valuable for an end‑user?\\n\\n6. **Agentic RAG incorporates autonomous action planning.**  \\n   – How does it differ from purely reactive RAG approaches?  \\n   – In what ways can it iterate on retrieval strategies throughout a task?\\n\\n7. **Describe the Self‑RAG loop.**  \\n   – After generation, what is stored for future retrieval and why is that beneficial?  \\n   – Illustrate a scenario where self‑retrieval improves the accuracy of subsequent responses.\\n\\n8. **For each RAG variant, identify one challenge or limitation that the architecture must overcome.**\\n\\n9. **Why might an organization opt for a Speculative or Fusion RAG variant rather than Standard RAG when working on a policy‑driven or regulatory compliance chatbot?**\\n\\n10. **In the context of a “Free Certification on RAGs” mentioned at the end of the document, what specific competencies or learning outcomes would you expect participants to attain?**  \\n    – Which RAG techniques would most likely be emphasized?\\n\\n*(These questions cover every element presented in the source – the definitions, step‑by‑step procedures, comparative benefits, practical scenarios, and the certification mention—ensuring none of the provided information is omitted.)*'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6e3099",
   "metadata": {},
   "source": [
    "### Generating Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6da90f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2351bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generation_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83f8285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  **Questions – RAG (Retrieval‑Augmented Generation) Primer**\n",
      "Answer:  **Retrieval‑Augmented Generation (RAG) Primer – Key Concepts, Types & How They Work**\n",
      "\n",
      "---\n",
      "\n",
      "### 1. What is RAG?\n",
      "\n",
      "RAG blends a **retrieval model** (e.g., a search engine) with a **generative model** (e.g., GPT‑style language model).  \n",
      "* Retrieval fetches relevant passages from a large database.  \n",
      "* The generative model incorporates this retrieved data to produce coherent, context‑aware content that addresses a user’s query.\n",
      "\n",
      "The core flow usually follows these stages:\n",
      "\n",
      "1. **Query Input** – user statement or question.  \n",
      "2. **Retrieval Process** – encode the query, perform vector search / dense retrieval across a corpus, pull back the most relevant documents.  \n",
      "3. **Select Top‑K** – rank documents by relevance (top 3–5 commonly used).  \n",
      "4. **Generate** – feed selected snippets to the generative model, which writes the final answer.  \n",
      "5. **Output** – present the synthesized answer to the user.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Variants of RAG\n",
      "\n",
      "| Variant | Core Idea | Main Additional Steps (from standard RAG) | How It Differentiates |\n",
      "|---------|-----------|-------------------------------------------|-----------------------|\n",
      "| **Standard RAG** | Basic retrieval + generation. | None extra. | The baseline approach, suitable when quick answers from a knowledge base are enough. |\n",
      "| **Corrective RAG** | Adds validation of the generated answer against a trusted source. | 1. Generate ➜ 2. Compare to authoritative data ➜ 3. Fix discrepancies ➜ 4. Iterate | Improves factual accuracy; useful for high‑stakes domains like healthcare or legal info. |\n",
      "| **Speculative RAG** | Generates multiple candidate responses. | 1. Retrieval ➜ 2. Multiple speculative responses ➜ 3. Scoring each with a feedback loop ➜ 4. Pick the highest score | Handles ambiguous queries or multiple valid interpretations, enhancing answer quality by choosing the best among plausible options. |\n",
      "| **Fusion RAG** | Integrates knowledge from several sources. | 1. Retrieval of diverse documents ➜ 2. Identify common themes & handle conflicting facts ➜ 3. Synthesize a single, balanced reply | Produces comprehensive, multi‑perspective answers; useful for complex, multidisciplinary topics. |\n",
      "| **Agentic RAG** | The AI drives its own retrieval strategy, acting like an autonomous agent pursuing a predefined goal. | 1. Define goal ➜ 2. Autonomous planning of queries & actions ➜ 3. Iterative retrieval + generation while adjusting strategy ➜ 4. Conclude when goal satisfied | Lets the model self‑direct retrieval steps to solve intricate tasks (e.g., tutoring, project planning). |\n",
      "| **Self RAG** | Self‑references past outputs for future retrieval, creating a loop of continuous improvement. | 1. Generate + store output ➜ 2. For future queries also search own past outputs ➜ 3. Learn patterns, correct biases ➜ 4. Keep iterating | Enables self‑learning over time without external dataset updates; helpful for adaptive personal assistants. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Typical End‑to‑End RAG Flow (with annotations)\n",
      "\n",
      "1. **Query Input**  \n",
      "   *What user asks.*\n",
      "\n",
      "2. **Retrieval Process**  \n",
      "   - Encode query and docs to high‑dimensional vectors.  \n",
      "   - Fetch documents (or passages) that best align with query semantics.\n",
      "\n",
      "3. **Selection of Top Documents**  \n",
      "   - Rank and pick the most relevant.  \n",
      "   - Number of docs can vary by system (e.g., k=5,10).\n",
      "\n",
      "4. **Generative Response**  \n",
      "   - Feed selected texts into language model.  \n",
      "   - Generate answer that “knows” about retrieved content, blending its general knowledge.\n",
      "\n",
      "5. **[Variant‑Specific] Validation / Refinement / Fusion**  \n",
      "   - Corrective: check against trusted data.  \n",
      "   - Speculative: score multiple drafts.  \n",
      "   - Fusion: merge multiple document perspectives.  \n",
      "   - Agentic: plan further queries if needed.  \n",
      "   - Self: pull previous similar answers for reference.\n",
      "\n",
      "6. **Output**  \n",
      "   *User sees final answer.*\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Typical Use‑Cases\n",
      "\n",
      "* **FAQ / Support systems** – quick and accurate replies backed by up‑to‑date knowledge bases.  \n",
      "* **Educational platforms** – tailored explanations that adapt to student questions.  \n",
      "* **Domain‑specific assistance** (medical, legal) – higher confidence via corrective or fusion layers.  \n",
      "* **Creative or exploratory AI** – speculative RAG to offer creative drafts and then refine.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Quick Checklist Before Deploying an RAG\n",
      "\n",
      "| Checklist Item | Why It Matters | What to Do |\n",
      "|-----------------|----------------|-------------|\n",
      "| Data Coverage | Ensure retrieval corpus has all needed topics | Regularly audit and expand database |\n",
      "| Retrieval Accuracy | Bad sources lead to bad generation | Tune vector similarity thresholds, add metadata filters |\n",
      "| Model Confidence | Generic model can hallucinate | Incorporate confidence‑based flags or confidence scoring |\n",
      "| Fact Checking | Risk in user-facing product | Add corrective layer or cross‑validate with trusted APIs |\n",
      "| Resource Efficiency | Retrieval + generation can be costly | Optimize indexing, consider batching |\n",
      "| Governance | Need to comply with privacy & regulations | Add data governance checks and explainability steps |\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Getting Started with RAG Development\n",
      "\n",
      "1. **Pick a retrieval framework** (FAISS, Elastic Search, ANN libraries).  \n",
      "2. **Define vectorisation method** – pre‑trained transformer embeddings typically suffice.  \n",
      "3. **Set up a retrieval–generation pipeline** – frameworks like 🤗 `Transformers + Datasets + RetrievalQA` can get you started.  \n",
      "4. **Experiment with variant layers** – start standard, add a correction checker (e.g., simple rule‑based comparison), and then optionally add speculative ranking or fusion.  \n",
      "5. **Monitor for hallucination and bias** – use automated metrics or manual A/B tests.  \n",
      "6. **Iterate** – leverage logs for Self‑RAG style refinement.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Where to Learn More\n",
      "\n",
      "* Look at papers from OpenAI and huggingface that describe RAG architectures.  \n",
      "* Explore example notebooks implementing standard RAG or the correction variant.  \n",
      "* If you’re just starting, many platforms now bundle ready‑to‑use RAG components (e.g., LangChain, Cohere’s Retrieval API).  \n",
      "\n",
      "---\n",
      "\n",
      "Feel free to dive deeper into any specific RAG type or step. This primer provides a snapshot of how the retrieval‑augmented generation landscape is structured and what makes each variant unique.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I didn’t see a question in your last message. How can I assist you today?\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  1. **What is Standard RAG and what are the two core components it combines?**  \n",
      "Answer:  **Standard RAG** is a Retrieval‑Augmented Generation method that blends two distinct abilities:  \n",
      "\n",
      "1. **Retrieval Model** – a search engine (or dense vector retriever) that fetches the most relevant documents or passages from a large corpus.  \n",
      "2. **Generative Model** – a language model (e.g., GPT) that uses the retrieved content to generate a coherent, context‑accurate response.  \n",
      "\n",
      "These two components—retrieval for accurate, up‑to‑date facts and generative text for natural language output—constitute the core of Standard Retrieval‑Augmented Generation.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     a) Explain how the retrieval model and generative model cooperate.  \n",
      "Answer:  **Standard Retrieval‑Augmented Generation (RAG): The Dance Between Two Models**\n",
      "\n",
      "In a Standard RAG system there are **two specialized components** that work together like a well‑orchestrated duet. Each has a very different skill set, but together they produce accurate, contextually grounded answers that a pure language model alone would struggle to give.\n",
      "\n",
      "| Phase | Retrieval Model | Generative Model |\n",
      "|-------|-----------------|------------------|\n",
      "| **Goal** | **\"Fetch what you need.\"** | **\"Turn that fetched info into an answer.\"** |\n",
      "| **What It Does** | 1. Encodes the user query into a dense vector.<br>2. Searches a vast corpus (a document store or an inverted index).<br>3. Ranks results, returns the *k* most relevant passages or documents. | 1. Takes the raw passages supplied by the retriever and concatenates them into a *context string*.<br>2. Feeds the (query + retrieved‑text) pair into a large generative transformer (often a variant of GPT‑3, GPT‑4, or the transformer models used in Facebook’s RAG).<br>3. Generates a **single** coherent response that should be grounded in the retrieved data. |\n",
      "| **Typical Flow** | • **User query** ➜ <br>• **Encode to vector** ➜ <br>• **Search index** ➜ <br>• **Return top‑*k*** documents (e.g., k = 5–10). | • **Query + top‑k docs** ➜ <br>• **Prompt the generative model** ➜ <br>• **Generate answer** (the model internally “knows” that it should anchor its words in the supplied passages). |\n",
      "| **Why It Works** | Retrieval turns an enormous unstructured database into a *compact, high‑relevance subset* that the generator only has to handle. That makes generation tractable and semantically trustworthy. | Generators are *good at language flow* and can handle the limited context (thanks to the Retriever having already trimmed the space). They can weave the factual sentences from the documents into natural, stylistic prose that users expect. |\n",
      "| **Result** | The user sees an answer that:<br>• directly refers to the exact content that was fetched<br>• reflects real evidence present in those documents, not just hallucinated facts<br>• can be as short or detailed as needed by the prompting scheme |\n",
      "\n",
      "**Practical Snapshot (from a real‑world implementation)**\n",
      "\n",
      "1. **Input:** \"Tell me the history of the Berlin Wall.\"\n",
      "2. **Retrieval:** Pulls 5 short excerpts: two from Wikipedia, two from a history journal article, one news clipping from 1961.\n",
      "3. **Generator Prompt:**\n",
      "\n",
      "   ```\n",
      "   [Context]\n",
      "   1) Berlin Wall was erected on 13th August 1961...\n",
      "   2) According to the German Historian X…\n",
      "   …\n",
      "\n",
      "   [Query] Tell me the history of the Berlin Wall.\n",
      "\n",
      "   **Answer:**\n",
      "   ...\n",
      "   ```\n",
      "4. **Output:** A neatly‑phrased narrative that cites specific dates and events extracted from the provided citations, giving the reader confidence that the statement is *grounded*.\n",
      "\n",
      "---\n",
      "\n",
      "**Take‑away:**\n",
      "\n",
      "- **Retrieval** = *Finding the right clues.*\n",
      "- **Generation** = *Merging the clues into a fluid, informative answer.*\n",
      "- Together, they create a system that is both knowledge‑accurate *and* linguistically polished, which is the power of standard Retrieval‑Augmented Generation.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     b) Provide an example of a use‑case where this architecture shines.\n",
      "Answer:  **Use‑case example: a *specialized medical symptom checker***\n",
      "\n",
      "**Context**\n",
      "\n",
      "A healthcare organization builds a patient‑facing chatbot that helps users assess symptoms, suggest possible conditions, and decide whether to seek medical care.\n",
      "\n",
      "**How RAG shines**\n",
      "\n",
      "| Stage | RAG Component | Why it matters |\n",
      "|-------|---------------|----------------|\n",
      "| 1. User asks, “I’ve had a fever and a sore throat for three days, can I skip seeing a doctor?” | **Retrieval** | The system pulls the latest guidelines from the Centers for Disease Control (CDC), peer‑reviewed papers on COVID‑19, and relevant WHO alerts. |\n",
      "| 2. GPT (or equivalent) uses the retrieved snippets to craft an answer that cites evidence (e.g., “According to the CDC…”, “A 2021 NEJM study found…”) and respects the user’s context (e.g., age, medical history). | **Generation** | Produces a coherent, personalized response that blends up‑to‑date information with a safety‑focused recommendation. |\n",
      "| 3. Validation (optional correctionary RAG) | If an error is detected—say the model mistakenly cites an outdated dosage— a trusted validator flags the issue, retrieves the corrected prescription guideline, and rewrites the answer. | Prevents the propagation of obsolete medical data. |\n",
      "| 4. User receives the final answer with references and a disclaimer, encouraging evidence‑based self‑care or professional consultation. | **Output** | Deliver reliable, up‑to‑date medical support without having all knowledge baked into the model. |\n",
      "\n",
      "**Why this architecture is ideal**\n",
      "\n",
      "* **Dynamic Knowledge:** The medical field evolves rapidly; RAG can fetch the newest evidence at request time without retraining the base language model.  \n",
      "* **Explanation & Transparency:** Retrieved snippets can be shown to the user (e.g., link to the CDC page), boosting trust and interpretability.  \n",
      "* **Error Handling:** Corrective or speculative RAG layers reduce hallucinations, ensuring critical health advice remains accurate.\n",
      "\n",
      "Thus, a **specialized, fact‑heavy domain such as medical symptom checking** benefits from RAG’s ability to combine up‑to‑date data, trusted references, and robust generation.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  It looks like you’ve posted a summary of various RAG techniques. Do you have a specific question about them—perhaps how to implement a particular type, compare the approaches, or a request for guidance on the certification program? Please let me know, and I’ll be happy to help!\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  2. **Walk through each step of the Standard RAG workflow (from query input to final output).**  \n",
      "Answer:  **Step‑by‑step walk‑through of the Standard Retrieval‑Augmented Generation (RAG) workflow**\n",
      "\n",
      "| Step | What happens | Key goal |\n",
      "|------|--------------|----------|\n",
      "| **1. Query input** | The user submits a question or prompt. | Capture the user's intent. |\n",
      "| **2. Retrieval process** | A retriever searches a large corpus (e.g., document collection, web indices) using vector or dense‑search methods to find passages most relevant to the query. | Pull in raw evidence that can help answer. |\n",
      "| **3. Selection of top documents** | The retriever ranks documents by relevance and picks the top‑*k* (often top‑5). These passages become the “knowledge base” the generator will use. | Provide the best, most useful snippets. |\n",
      "| **4. Generative response** | The retrieved snippets are fed into the language model (GPT‑style). Using both the prompt and the context, the model crafts a coherent answer that reflects the retrieved information. | Merge human prompt and retrieved facts into one finished answer. |\n",
      "| **5. Output** | The model returns the final response to the user, enriched by the pulled documents for accuracy and detail. | Deliver a fact‑backed, context‑aware answer. |\n",
      "\n",
      "That’s the standard pipeline: **(Query) → (Retrieve) → (Pick top docs) → (Generate + context) → (Output)**.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     – For every step, name the typical methods used (e.g., vector search, dense retrieval).\n",
      "Answer:  Below is a “tool‑kit” for the *most common methods* you’ll encounter (or use yourself) at **every phase** of the six major RAG styles described in the material you gave me.  \n",
      "The entries are deliberately kept short but **actionable** – you can mix, match, or adapt them to whatever particular RAG variant you’re engineering.\n",
      "\n",
      "| **RAG Variant / Phase** | **Typical Methods & Technologies** |\n",
      "|---------------------------|-------------------------------------|\n",
      "| **Standard RAG** | **(1) Query Input** – nothing special; any text encoder can be used to tokenize/normalize the user’s question. |\n",
      "| | **(2) Retrieval Process** – <ul><li>Vector search / dense retrieval (FAISS, Pinecone, Milvus).<br><li>Traditional IR: BM25 or Lucene/Kuery‑by‑Example if the corpus is highly structured. </ul> |\n",
      "| | **(3) Selection of Top Documents** – <ul><li>Top‑k ranking using cosine similarity, Jaccard, or learned rerankers.<br><li>Maximal marginal relevance (MMR) for diversity, or simple cutoff thresholds. </ul> |\n",
      "| | **(4) Generative Response** – <ul><li>Transformer‑based generation (GPT‑4, LLaMA 2, Mixtral).<br><li>Prompt‑engineering, few‑shot or chain‑of‑thought generation. </ul> |\n",
      "| | **(5) Output** – Plain text, optionally formatted (markdown, code fences). |\n",
      "| **Corrective RAG** | **(1) Initial Retrieval & Generation** – same as Standard RAG above. |\n",
      "| | **(2) Validation Process** – <ul><li>Cross‑check against trusted repositories (PubMed, government API, verified knowledge base).<br><li>Rule‑based heuristics or pattern matching for consistency. </ul> |\n",
      "| | **(3) Correction Mechanism** – <ul><li>Re‑run the generator conditioned on “please fix the last response” + validity flags.<br><li>Fine‑tune a “clarifying” decoder that only updates parts that conflict. </ul> |\n",
      "| | **(4) Iteration & Feedback Loop** – <ul><li>Re‑ranking of candidate answers by a dedicated validation scorer.<br><li>Loop limit (e.g., 2–3 turns) or confidence threshold triggers end. </ul> |\n",
      "| | **(5) Final Output** – Same format as Standard RAG, with a *confidence indicator* if desired. |\n",
      "| **Speculative RAG** | **(1) Retrieval** – vector search or dense retrieval (same as Standard RAG). |\n",
      "| | **(2) Generative Speculation** – <ul><li>Beam search or nucleus (top‑p) sampling to produce several distinct hypotheses.<br><li>Decoding with “temperature” set high (e.g., 1.0–1.5) to increase variability. </ul> |\n",
      "| | **(3) Feedback & Ranking** – <ul><li>Automated evaluation: ROUGE, BERTScore, or learned answer‑faithfulness classifiers. <br><li>External reranker (e.g., BM25 over generated tokens against the original corpus). </ul> |\n",
      "| | **(4) Selection Process** – <ul><li>Choose highest‑scored hypothesis, optionally tie‑breaking with human‑in‑the‑loop. <br><li>Use confidence‐calibrated logistic regression for final choice. </ul> |\n",
      "| | **(5) Presentation** – Output the best hypothesis; optionally show the alternatives/uncertainty heat‑map. |\n",
      "| **Fusion RAG** | **(1) Retrieval of Diverse Documents** – <ul><li>Multiple back‑end engines (dense, sparse, structured).<br><li>Document partitioning, e.g., paragraph‑level vs. article‑level. </ul> |\n",
      "| | **(2) Information Integration** – <ul><li>Encoder–Decoder models (e.g., GPT‑3) with “multi‑source” prompts. <br><li>Neural fusion (cross‑attention over all passages), or rule‑based merging. </ul> |\n",
      "| | **(3) Synthesis of Unified Response** – <ul><li>Multi‑source prompting: “Summarise all you learned.” <br><li>Controversy‑aware generation (include “according to X…”) for balanced answers. </ul> |\n",
      "| | **(4) Conflict Resolution** – <ul><li>Credibility weighting: source scores or evidence scores from fact‑checking APIs.<br><li>Score‑based conflict resolution (e.g., “pick the value with majority support”). </ul> |\n",
      "| | **(5) Output** – Consolidated response, optionally citing sources or providing provenance URLs. |\n",
      "| **Agentic RAG** | **(1) Goal Definition** – define intent; optionally translate into a high‑level prompt or a decision‑tree skeleton. |\n",
      "| | **(2) Autonomous Action Planning** – <ul><li>Goal‑oriented prompt templates (“You must gather facts X, Y, Z”). <br><li>RL‑based action selection if iterative environment. </ul> |\n",
      "| | **(3) Iterative Retrieval & Generation** – <ul><li>Loop of “search→gen→evaluate” until the goal score > threshold.<br><li>Dynamic plan re‑evaluation each cycle. </ul> |\n",
      "| | **(4) Dynamic Adjustment** – <ul><li>Update retrieval index (e.g., insert new docs generated during the cycle). <br><li>Adapt prompt tone based on success/failure signals. </ul> |\n",
      "| | **(5) Completion of Goal** – Confirm fulfillment (e.g., answer accepted or all subtasks satisfied), then provide final output. |\n",
      "| **Self‑RAG** | **(1) Initial Retrieval & Generation** – as Standard RAG. |\n",
      "| | **(2) Self‑Storage** – <ul><li>Store generated chunks in a local vector store (Pinecone, Weaviate).<br><li>Tag with metadata (timestamp, confidence). </ul> |\n",
      "| | **(3) Iterative Self‑Retrieval** – <ul><li>Use the store as a “secondary” retrieval source along with external docs.<br><li>Meta‑retrieval: embed previous generation vectors for self‑referencing. </ul> |\n",
      "| | **(4) Self‑Improvement Loop** – <ul><li>Compare latest answer vs. prior similar queries, use error patterns to adjust retrieval weights.<br><li>Continual learning: fine‑tune a retrieval re‑ranker on past Q/A pairs. </ul> |\n",
      "| | **(5) Enhanced Output** – Provide response that leverages both external knowledge & learned from its own prior generations, possibly with a provenance map of internal vs. external source. |\n",
      "\n",
      "> **How to pick the methods for your own system**  \n",
      "> 1. Identify which *phase* matters most for your goal (explain “do we just fetch, or do we need multi‑source reasoning?”).  \n",
      "> 2. Start with *standard* dense retrieval (FAISS, HNSW).  \n",
      "> 3. For quality: add *reranking* (learned reranker or MMR).  \n",
      "> 4. For correctness: apply a *validation* step (facts checker) as in Corrective RAG.  \n",
      "> 5. If you anticipate ambiguous prompts, consider **Speculative RAG** – generate multiple replies and rank.  \n",
      "> 6. For complex tasks requiring multi‑perspective knowledge, adopt a **Fusion RAG** approach.  \n",
      "> 7. If your system itself must perform ongoing research, use **Agentic** or **Self‑RAG** mechanisms.\n",
      "\n",
      "Feel free to let me know which particular RAG type you’re building, and we can drill into concrete libraries (e.g., `sentence‑transformers`, `FAISS`, `Flax GPT`, `OpenAI API`, etc.) or API‑code patterns to deploy.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I’m not sure I caught a question here—could you let me know what you’d like help with?\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  3. **Explain the objective of a Corrective RAG system and the main difference between it and Standard RAG.**  \n",
      "Answer:  **Objective of a Corrective RAG system**\n",
      "\n",
      "- **To ensure factual correctness** – After a retriever pulls relevant passages and the generator drafts an answer, the Corrective RAG verifies that the generated text aligns with a *trusted* knowledge source (e.g., a reputable database, scholarly articles, or verified news outlets).\n",
      "- **To automatically repair mistakes** – If the validation stage finds inconsistencies or errors, the model corrects the draft (generating a refined answer or a fresh response) so that the final output is trustworthy and accurate.\n",
      "\n",
      "**Key difference from Standard RAG**\n",
      "\n",
      "| Aspect | Standard RAG | Corrective RAG |\n",
      "|--------|--------------|----------------|\n",
      "| **Validation step** | None – the generated answer is accepted as is. | Included – a secondary “verification & correction” loop compares the answer to trusted data. |\n",
      "| **Result fidelity** | Quality depends mainly on the generator’s own knowledge; risk of hallucinations. | Higher factual reliability; erroneous content is detected and corrected before it reaches the user. |\n",
      "| **Process flow** | Retrievable docs → One‑shot generation → Output. | Retrievable docs → Generation → *Validate* → *If needed: correct and re‑generate* → Output. |\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     – Describe the validation step: what kinds of sources could serve as “trusted datasets”?  \n",
      "Answer:  ## Validation Step in Corrective RAG  \n",
      "Once the generative model has produced a provisional answer, Corrective RAG performs a **validation stage**. In this phase the answer is checked against one or more **trusted datasets** that are known to be reliable, authoritative, and up‑to‑date. If inconsistencies are detected, the system can regenerate or refine the response before sending it back to the user.\n",
      "\n",
      "### Typical sources that can serve as “trusted datasets”\n",
      "\n",
      "| Domain | Example Trusted Data Sources | Why they’re trusted |\n",
      "|--------|-----------------------------|---------------------|\n",
      "| **General knowledge & encyclopedias** | Wikidata (quality‑controlled version), curated ontologies | Widely maintained, structured, and linked to primary facts |\n",
      "| **Academic / scientific content** | PubMed/PMC for biomedical literature, IEEE Xplore, ACM Digital Library, arXiv (reviewed), Journal databases (Nature, Science) | Peer‑reviewed research; vetted by domain experts |\n",
      "| **Government & official statistics** | U.S. Census Bureau, Eurostat, World Bank data, WHO datasets, official legislative databases (e.g., U.S. Code, EU directives) | Released by public authorities, legally mandated reporting |\n",
      "| **Healthcare guidance** | Clinical Practice Guidelines (e.g., NICE, CDC, WHO), medical databases like MedlinePlus, UpToDate, guideline repositories (guideline.gov) | Synthesized by expert panels, regularly updated |\n",
      "| **Legal & regulatory frameworks** | ISO standards, IEEE standards, GDPR provisions, statutory laws, case law databases | Codified norms with formal legal authority |\n",
      "| **Industry‑specific best practices** | SAP, Salesforce, SAP Knowledgebase for enterprise software; For aerospace, FAA regulations; For finance, FOMC releases, SEC filings | Compiled by industry bodies and governing institutions |\n",
      "| **High‑trust news outlets** | BBC, The Wall Street Journal, Reuters, The Associated Press | Established reputation for journalistic integrity, fact‑checking protocols |\n",
      "| **Open‑source knowledge bases** | DBpedia, OpenStreetMap (geospatial data), Wikidata | Peer‑maintained but with strong edit oversight and quality rules |\n",
      "\n",
      "### How validation typically works\n",
      "\n",
      "1. **Retrieve the most relevant subset of a trusted dataset** given the original query and/or the candidate answer.  \n",
      "2. **Check factual claims** – for every key fact in the answer compare it against the matched records from the trusted source.  \n",
      "3. **Resolve discrepancies** – if a claim conflicts with the trusted data, the system either pulls a more reliable source, re‑generates the passage, or tags it as uncertain.  \n",
      "4. **Rank/score confidence** – combine alignment metrics (e.g., exact keyword match, semantic similarity) with source provenance to estimate how trustworthy the final answer is.  \n",
      "5. **Iterate if needed** – loop back to steps 1‑3 until the answer aligns within an acceptable error margin or falls under a confidence threshold.\n",
      "\n",
      "In practice, the validation step often employs *structured* trusted data (e.g., relational tables, semantic graphs) because those formats map easily onto specific statements. For domains where high‑quality structured data are unavailable (like open‑ended narratives), the system may resort to *peer‑reviewed texts* (journals) or *reputable news summaries* as surrogate “trusted” references.\n",
      "\n",
      "---\n",
      "\n",
      "By basing validation on one or more of the above‑listed source categories, Corrective RAG gains the ability to cross‑check model outputs against independently verified facts and thus produce more accurate and trustworthy responses.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     – How does the correction mechanism refine or regenerate a response?\n",
      "Answer:  In the **Corrective RAG** workflow the “correction mechanism” works like a self‑checking loop that keeps refining an answer until it aligns with trustworthy information. It generally follows these steps:\n",
      "\n",
      "1. **Generate a first draft** – After the retriever pulls the top‑k relevant documents, the generative model builds an answer using that content.\n",
      "\n",
      "2. **Validate against a trusted source** – The output is then compared (typically automatically) to a “trusted” dataset or source such as an authoritative database, verified research papers, or reputable news outlets. This can involve surface‑level matching of facts or deeper semantic alignment.\n",
      "\n",
      "3. **Identify discrepancies** – If the comparison finds mismatches or missing data, the system flags these issues. This could be anything from an incorrectly cited statistic to a mis‑interpreted technical term.\n",
      "\n",
      "4. **Apply corrections** – Once a problem is detected, the system can:\n",
      "   * **Retrigger retrieval** for additional or higher‑quality passages that resolve the inconsistency.\n",
      "   * **Rewrite the problematic portion** using corrected information from the newly fetched source.\n",
      "   * **Re‑run the generator** on the updated context to produce a polished, fixed answer.\n",
      "\n",
      "5. **Iterate until acceptable** – Steps 2‑4 repeat in a loop, continually refining the answer until no significant discrepancies remain, or until a user‑defined error tolerance threshold is met.\n",
      "\n",
      "6. **Return the validated answer** – The final, corrected response is then presented to the user.\n",
      "\n",
      "In short, the mechanism oscillates between retrieval, generation, validation, and regeneration, using detected errors as signals to drive the next refinement cycle.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I didn’t see a question in your message. If there’s something specific you’d like to ask or discuss about RAG or anything else, feel free to let me know!\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  4. **Speculative RAG attempts to handle ambiguity or multiple interpretations.**  \n",
      "Answer:  Yes, that is a correct description of **Speculative Retrieval‑Augmented Generation (Speculative RAG)**. The key idea behind Speculative RAG is that, unlike traditional RAG which generates a single response based on a set of retrieved documents, it deliberately creates a *set* of possible responses. Each candidate is then evaluated—usually by a secondary scoring or filtering mechanism—to decide which one best fits the user’s intent. This process naturally accommodates:\n",
      "\n",
      "* **Ambiguous queries** – where there may be several valid answers.\n",
      "* **Multiple interpretations** – by allowing the model to explore distinct perspectives before selecting the most suitable one.\n",
      "* **Uncertainty handling** – where the system can showcase several plausible explanations and choose the most coherent or factually correct option.\n",
      "\n",
      "By generating multiple hypotheses and ranking them, Speculative RAG improves the robustness of the system in situations where a single retrieved document or straightforward answer might miss subtle nuances or conflicting information.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     – List the steps taken from retrieval to the final selection of a response.  \n",
      "Answer:  **Steps from Retrieval to the Final Selection of a Response**\n",
      "\n",
      "1. **Query Input** – The user’s question is fed into the system.  \n",
      "2. **Retrieval** – A retrieval model searches a large corpus and fetches passages most relevant to the query (e.g., top‑k documents).  \n",
      "3. **Ranking & Selection** – Those passages are scored for relevance, and the best k are chosen.  \n",
      "4. **Generation (Optional — multiple or iterative)**  \n",
      "   - *Standard RAG*: Single‑pass generation uses the retrieved texts to compose a coherent reply.  \n",
      "   - *Speculative / Fusion RAG*: The model produces several candidate replies, often integrating multiple source excerpts.  \n",
      "   - *Corrective RAG*: The draft reply is compared against trusted references and corrected as needed.  \n",
      "   - *Agentic / Self RAG*: The model may iteratively query again, refining the answer until it satisfies the goal.  \n",
      "5. **Evaluation & Scoring** – Generated candidate replies (when multiple are created) are scored on relevance, coherence, factual accuracy, etc.  \n",
      "6. **Selection of Final Response** – The best‑scored candidate is chosen, or the corrected/ refined version is adopted.  \n",
      "7. **Output** – The selected, finalized answer is returned to the user.  \n",
      "\n",
      "These steps collectively move from a raw query, through retrieval and optional iterative improvement, to the point where the system delivers the most suitable response.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     – How is the feedback‑based ranking of the speculative answers usually implemented?\n",
      "Answer:  In Speculative Retrieval‑Augmented Generation, once the generator produces several candidate responses, a **feedback‑based ranker** selects the best one. The ranking is almost always done automatically inside the pipeline, using one of the following mechanisms:\n",
      "\n",
      "| Ranker type | How it works | Typical implementations |\n",
      "|-------------|--------------|--------------------------|\n",
      "| **Linguistic or heuristic scorer** | The system uses hand‑crafted rules and NLP metrics (e.g., relevance, fluency, length, grammatical correctness). | • Cosine similarity between the query and a candidate. <br>• ROUGE or BLEU scores against retrieved documents. <br>• Token‑count or diversity heuristics. |\n",
      "| **Neural embedder** | Each candidate and the query/document passages are encoded into vectors, and a similarity score is computed. | • Sentence‑Transformer models (e.g., cross‑encoder, dual‑encoder).<br>• Custom dense‐retrieval head trained as a “score‑model”. |\n",
      "| **Metric‑learning ranker** | A lightweight classifier or ranking neural network predicts a score for each candidate. | • Train on pairs (good vs. bad responses) using human or proxy labels. |\n",
      "| **Feedback from a LLM** | An additional language model (or a “judge” model) scores each candidate on factuality, relevance, or safety. | • Prompting GPT‑4 to output a score in natural language then parsing. <br>• Using an internally trained classifier that mimics GPT‑style judgment. |\n",
      "| **Relevance‑plus‑coverage oracle** | The system measures overlap of factual assertions between a candidate and multiple retrieval passages. | • TF‑IDF overlap, FactCC, or Graph‑based entailment checks. |\n",
      "\n",
      "#### Typical pipeline\n",
      "\n",
      "1. **Generate speculations** – the main generation model produces *N* outputs (e.g., N = 10).  \n",
      "2. **Retrieve secondary evidence** – the retriever can be queried again with each candidate to fetch “supporting” passages.  \n",
      "3. **Compute scores**  \n",
      "   * For every candidate, calculate  \n",
      "     * *Relevance*: cosine similarity with retrieval embeddings or with the question.  \n",
      "     * *Factual consistency*: overlap with high‑confidence passages or a judgment model.  \n",
      "     * *Coherence & style*: simple heuristics or a finetuned classifier.  \n",
      "4. **Normalize / weigh scores** – assign weights according to task (e.g., give 60 % weight to factuality).  \n",
      "5. **Rank & pick** – choose the candidate with the highest aggregate score as the final answer.  \n",
      "\n",
      "#### In practice\n",
      "\n",
      "- **Pre‑trained rankers** (cross‑encoders trained on MS MARCO or Natural Questions) are common; they can directly output relevance scores.\n",
      "- **LLM “judge” modules** are becoming popular. A secondary call to the same generative model or to a smaller evaluator model scores each answer in a prompt:  \n",
      "  ```\n",
      "  For each of the following responses, rate relevance 0-5:\n",
      "  Response 1: …\n",
      "  Response 2: …\n",
      "  …\n",
      "  ```\n",
      "\n",
      "- The chosen response may be further post‑processed (e.g., trimming, formatting) before returning to the user.\n",
      "\n",
      "In short, the feedback‑based ranking is typically a quick, automated comparison step that scores each speculative answer on relevance, coherence, factuality, and sometimes style, using vector similarity, supervised classifiers, or a lightweight LLM‑judge, and picks the top‑scoring candidate.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I’m not sure what you’d like to know. Could you please tell me your specific question or how I can help you with RAGs?\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  5. **What distinguishes Fusion RAG from the previous variants?**  \n",
      "Answer:  Fusion RAG is built for “**multi‑source synthesis**” rather than the other methods’ focus points.  \n",
      "Here’s what makes it distinct:\n",
      "\n",
      "| Feature | Standard RAG | Corrective RAG | Speculative RAG | Self/Agentic RAG | **Fusion RAG** |\n",
      "|--------|--------------|----------------|----------------|------------------|---------------|\n",
      "| **Input to generator** | One (or a few) best‑ranked documents | One set of documents + a validation pass | Many documents → multiple speculative drafts | The model decides what to fetch and what next step to take (self‑guided) | **Diverse set of documents from many viewpoints** |\n",
      "| **Goal** | Answer the user’s query as closely as possible | Validate and *correct* a generated answer | Pick the most plausible of several generated responses | Reach an autonomous objective (plan, iterate) | **Produce a single, balanced response that blends** <br>multiple sources, highlighting agreements & contradictions |\n",
      "| **Processing** | “Retrieve → generate” | “Retrieve → generate → validate → (re‑generate if needed)” | “Retrieve → generate many options → rank → choose” | “Plan → retrieve → generate → refine” | “Retrieve diverse docs → identify common & conflicting facts → synthesize → conflict resolution → one unified reply” |\n",
      "| **Key Strength** | Speed & simplicity | Higher factual reliability | Creativity under ambiguity | Adaptivity & autonomy | *Holistic integration* – pulls in all relevant viewpoints and explicitly reconciles differences. |\n",
      "\n",
      "### In practice\n",
      "* **Fusion RAG fetches multiple, complementary passages.**  \n",
      "* It reads each, extracts common themes, detects contradictions, weighs evidence or source credibility, then merges the information into one coherent narrative.  \n",
      "* If there’s disagreement, it reports it or resolves it per established rules, producing a “well‑rounded” answer.\n",
      "\n",
      "That is the main distinguishing point: it’s not just about fetching and generating – it’s about *integrating, reconciling, and presenting* diverse evidence in a unified reply.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     – Detail how the system deals with **conflicting information** in the retrieved documents.  \n",
      "Answer:  When the RAG pipeline retrieves several documents that provide different or contradictory facts, the system follows a series of steps designed to surface a single, trustworthy answer.  \n",
      "Below is a concise outline of how a typical RAG implementation (drawing on the “Fusion RAG” model described in the supplied material) deals with such conflicts:\n",
      "\n",
      "| Stage | What the system looks for | What it does | Why it matters |\n",
      "|-------|--------------------------|--------------|----------------|\n",
      "| 1. **Detection of Disagreement** | – Comparison of key entities or facts across retrieved passages (e.g., “Population = 50M” vs. “Population = 70M”).  <br>– Simple string‑level or embedding‑level distance tests to flag statements that vary significantly. | The system tags passages with **“conflict indicators.”** | It knows when the data space is inconsistent. |\n",
      "| 2. **Source Credibility Assessment** | – Metadata (URL, domain, publication date). <br>– A pre‑trained **trust score** model that estimates reliability based on source type (government, peer‑reviewed journal, user forum, etc.). | Assigns a **numeric credibility weight** to every document. | Conflicts can be resolved by favouring the higher‑credibility source. |\n",
      "| 3. **Content‑Based Weighting** | – Extract the *semantic importance* of the claim (e.g., an answer about a public policy will weigh statements from authoritative bodies higher than opinions).<br>– Use the generation model’s *attention scores* to judge how central the claim is to the passage. | Combines the credibility weight with the claim‑importance score into a single **conflict‑resolution weight**. | It prevents a recent, but less reliable, statement from overriding a well‑grounded fact. |\n",
      "| 4. **Evidence‑Synthesis / Consensus Engine** | – Build a small “knowledge graph” from the conflict‑tagged facts. <br>– Identify **majority‑vote** or **symmetric consensus** across the top‑scoring documents. | Chooses the value that is agreed upon by the highest‑weight documents or that sits within the tightest range of values when the system can tolerate numeric bounds. | It gives the user the most robust fact rather than a single potentially wrong datum. |\n",
      "| 5. **Fallback / Speculation (Optional)** | – If no single value is strongly supported, the generator may create a **conditional statement** (e.g., “Sources disagree on whether …, but the most credible ones say …”). | Uses the language model to translate the conflict resolution evidence into natural language that acknowledges uncertainty. | Prevents hallucination by transparently communicating that information is disputed. |\n",
      "| 6. **Iterative Retrieval & Re‑Ranking** | – If after the above step a definitive answer is still lacking, the system can **re‑run retrieval** with more selective queries (“Population 2024 verified”). | Allows the loop to focus the next round on documents that are more likely to settle the dispute. | Maintains an adaptive strategy until enough evidence converges. |\n",
      "\n",
      "---\n",
      "\n",
      "### How Each RAG Type Interacts with Conflict\n",
      "\n",
      "| RAG Type | Conflict‑Handling Traits |\n",
      "|----------|------------------------|\n",
      "| **Standard RAG** | Uses the retrieved docs as‑is; resolution depends solely on how the generation model blends them (often implicit weighting via attention). |\n",
      "| **Corrective RAG** | After generating a candidate answer, it cross‑checks the specific fact with a *trusted* dataset; if discrepancy found, it forces the model to regenerate using that verifiable source, effectively discarding less credible conflicting docs. |\n",
      "| **Speculative RAG** | Generates *multiple* plausible interpretations; if a claim appears in more of the top‑scoring generations, it is favored, thereby implicitly smoothing over outlier conflicting snippets. |\n",
      "| **Fusion RAG** | Explicitly contains a **Conflict‑Resolution** step that does most of the heavy lifting shown in the table above. |\n",
      "| **Agentic RAG** | Can query its own retrieval policy to prioritize more reliable sources, or iterate its planning steps until it obtains a single coherent view. |\n",
      "| **Self RAG** | As it stores its own prior outputs, future retrieval includes its self‑generated reconciled facts, reinforcing the resolved conflict over time. |\n",
      "\n",
      "---\n",
      "\n",
      "### Practical Tips (for developers who are implementing or customizing a RAG system)\n",
      "\n",
      "1. **Keep a dedicated “trusted‑sources” whitelist** that can automatically override conflicts.  \n",
      "2. **Cache “conflict summaries”** after the first pass so that if the same contradictory facts appear later, the system can pull a pre‑resolved answer instead of re‑running the whole process.  \n",
      "3. **Expose confidence scores** (from the credibility weighting) back to the user interface so that the audience can see how decisive the consensus is.  \n",
      "\n",
      "By layering these steps – from simple detection to source‑based weighting and, when necessary, an iterative clarification cycle – a RAG system robustly neutralizes contradictory information and presents the user with the most reliable answer it can formulate.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     – Why is integrating “diverse perspectives” valuable for an end‑user?\n",
      "Answer:  Integrating “diverse perspectives” is valuable for an end‑user because it allows the system to provide a more **complete, balanced, and trustworthy answer**:\n",
      "\n",
      "| Why it matters | How diversity helps |\n",
      "|-----------------|---------------------|\n",
      "| **Comprehensive coverage** | Different documents can cover separate facets of the same topic (e.g., legal, economic, technical, personal). The generated answer can touch on each of these dimensions, giving the user a fuller picture. |\n",
      "| **Reduced bias and blind spots** | Relying on a single source or viewpoint can unintentionally reinforce one narrative. By drawing from multiple sources, the model mitigates that bias and signals to the user that alternative views exist. |\n",
      "| **Conflict resolution** | When sources disagree, having a range of inputs lets the system compare them and reconcile contradictions—or at least report where they appear—so the user is not misled. |\n",
      "| **Credibility & relevance** | Diverse input raises the likelihood that the key facts are corroborated. The model can cite stronger, more authoritative sources and highlight the evidence behind conclusions. |\n",
      "\n",
      "In short, by synthesizing information from diverse documents, a Fusion RAG—or any retrieval‑augmented model—offers a richer, more nuanced answer that empowers users to make better-informed decisions.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  It looks like your message didn't come through. If you have a question or need information about RAG techniques, feel free to let me know and I'll help!\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  6. **Agentic RAG incorporates autonomous action planning.**  \n",
      "Answer:  That statement is correct. Agentic Retrieval‑Augmented Generation (Agentic RAG) is designed around the idea that the model not only fetches and generates, but does so autonomously: it sets a goal, plans what data to retrieve, iterates on its plan based on retrieval results, adjusts its strategy on the fly, and stops when the goal has been satisfied. This autonomy—making decisions and steering its own information‑gathering loop—is a core part of Agentic RAG.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     – How does it differ from purely reactive RAG approaches?  \n",
      "Answer:  **Reactive vs. Proactive (Agentic / Self‑RAG) approaches**\n",
      "\n",
      "| Aspect | Purely Reactive RAG (e.g., Standard RAG) | Agentic / Self‑RAG (proactive) |\n",
      "|--------|------------------------------------------|--------------------------------|\n",
      "| **Trigger** | A user prompt directly feeds the retriever → one batch of documents. | A predefined **goal** or task (explanation, tutoring, complex problem‑solving) is supplied. |\n",
      "| **Retrieval strategy** | Static – one retrieval pass (“fetch‑generate”) to the top‑k documents, then stop. | *Dynamic* – the model chooses what to query next, which sub‑goals to pursue, and may alter its query set based on what it learns in each round. |\n",
      "| **Generation cycle** | One shot: the generative model uses the retrieved context to produce a single answer. | Multi‑round: it may generate intermediate decisions or questions, feed those back into retrieval, and keep refining until a goal is met. |\n",
      "| **Decision‑making** | None – the model passively uses the documents it got. | Autonomous planning: the model sets sub‑steps, evaluates progress, adjusts the retrieval or generation plan on the fly. |\n",
      "| **Self‑feedback** | No mechanism; errors remain uncorrected unless the user intervenes. | Corrective steps can be introduced (e.g., Validation in Corrective RAG) or self‑storage of outputs (Self‑RAG), enabling iterative learning from its own previous responses. |\n",
      "| **Outcome** | Fast, single‑turn, and often adequate for clear queries but can miss nuance or resolve conflicts. | Delayed but potentially more accurate, balanced, or contextually rich, especially for multi‑faceted or ambiguous problems. |\n",
      "| **Use cases** | Quick fact‑checking, simple QA. | Complex reasoning tasks, learning scenarios, tutoring, multi‑perspective synthesis, dynamic decision making. |\n",
      "\n",
      "### Bottom‑line\n",
      "*Purely reactive RAG* stops after the first retrieval + generation: it responds “reactively” to the current user query, relying only on externally retrieved evidence.  \n",
      "*Agentic / Self‑RAG* operates *proactively*: it plans, refines, validates, or self‑stores information as it works toward a specified goal. This makes it more robust for problems that require iterative search, resolution of conflicting evidence, or continuous refinement of the answer.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     – In what ways can it iterate on retrieval strategies throughout a task?\n",
      "Answer:  **Retrieval strategies that a RAG system can cycle through during a task**  \n",
      "\n",
      "| Strategy | How the system loops/updates it |\n",
      "|----------|---------------------------------|\n",
      "| **Dynamic query reformulation** | After the generator produces a draft answer, the system can generate a *new query* (or “follow‑up questions”) that clarifies uncertainty or follows on from information it just produced. That revised query is fed back to the retriever, and fresh documents are fetched. |\n",
      "| **Multi‑round retrieval** | The system can be instructed to fetch *additional* documents after an initial round, possibly with higher relevance thresholds or narrower scopes, to fill in missing facts or resolve ambiguities uncovered in the first pass. |\n",
      "| **Iterative refinement with a validation loop (Corrective RAG)** | Each generated response is checked against a trusted database. If a mismatch is detected, the system can *re‑query* with constraints (e.g., “provide citations from PubMed” or “limit to authoritative sources”). The newly retrieved materials become the basis for a corrected answer. |\n",
      "| **Speculative generation plus feedback ranking (Speculative RAG)** | The generative model produces several alternative snippets based on the initial retrieval set. Each snippet is fed into an evaluator that may, in turn, ask the retriever for *extra documents* relevant to the speculative content that scored low. The loop can continue until the evaluator flags an answer as satisfactorily confident. |\n",
      "| **Fusion with source‑credibility weighting (Fusion RAG)** | The system may revise its retrieval weights if it discovers a source with higher trustworthiness or detect divergent viewpoints. After integration, it may query the same topic but *filter* out lower‑confidence documents to keep the synthesis focused. |\n",
      "| **Goal‑driven planning and retrieval (Agentic RAG)** | A pre‑defined objective (e.g., \"explain concept X\") initiates a plan: step 1 ask “What does X mean?” → retrieve → generate. If the generated explanation is too shallow, the planner can add an intermediate sub‑objective (“what are its real‑world examples?”) and re‑issue a tailored query. This loop continues until the objective is met. |\n",
      "| **Self‑referential improvement (Self RAG)** | Each output becomes a “datum” in the system’s personal corpus. A later query can therefore pull from both the original corpus *and* its past answers, refining the next retrieval. The system iteratively learns which of its earlier responses were most useful and re‑uses or adjusts those during future tasks. |\n",
      "\n",
      "**Key take‑away**: A RAG system can iterate on retrieval by repeatedly re‑crafting queries, altering relevance thresholds, requesting additional evidence, evaluating partial outputs against trusted data, re‑ranking speculative answers, weighting sources, or by embedding new user or internal goals. Each iteration refines the evidence base and nudges the generator toward a more accurate, comprehensive result.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I didn’t see a question in your last message. If you have something about Retrieval-Augmented Generation or anything else in mind, just let me know!\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  7. **Describe the Self‑RAG loop.**  \n",
      "Answer:  **The Self‑RAG loop** is a self‑referential retrieval‑augmented generation mechanism that lets an LLM use its own prior outputs as a growing knowledge base for future queries.  Its operation follows five key stages:\n",
      "\n",
      "| Stage | What Happens |\n",
      "|------|--------------|\n",
      "| **1. Initial Retrieval and Generation** | The user query is given to the retriever, which fetches relevant passages from the main corpus. The retrieved content is fed to the generative model to produce an answer. |\n",
      "| **2. Self‑Storage** | The answer created in step 1 is stored in a dedicated repository as a new “document” or data point. |\n",
      "| **3. Iterative Self‑Retrieval** | In subsequent turns, the retriever pulls not only from the original database but also from the stored self‑generated documents. This makes the system able to draw on its own past output. |\n",
      "| **4. Self‑Improvement Loop** | As more data accrue, the model detects patterns, gaps, or inconsistencies in its prior responses, and adjusts its retrieval strategy and generative style to compensate. |\n",
      "| **5. Enhanced Output** | The loop results in responses that grow more accurate and contextually aware over time because the model is continually learning from the very material it produced. |\n",
      "\n",
      "Thus, Self‑RAG creates a continual self‑training cycle where the AI enriches its knowledge base with its own earlier outputs and uses that enriched repository to better answer future questions.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     – After generation, what is stored for future retrieval and why is that beneficial?  \n",
      "Answer:  **What’s stored?**  \n",
      "In *Self RAG*, the system keeps the **own generated output** (the response text, and potentially the internal representation of that output) in a dedicated repository after it has been produced.\n",
      "\n",
      "**Why is that beneficial?**  \n",
      "\n",
      "| Benefit | How it helps |\n",
      "|---|---|\n",
      "| **Continuously enriched knowledge base** | New self‑generated passages become part of the corpus, expanding what the system can retrieve on future queries. |\n",
      "| **Self‑validation and correction** | By re‑retrieving its own outputs, the model can detect inconsistencies or errors, then re‑generates or refines them. |\n",
      "| **Personalized consistency** | Reusing prior answers keeps the assistant’s style and facts consistent over longer interactions. |\n",
      "| **Reduced hallucinations** | The AI can cross‑check its current answer against earlier vetted responses, lowering the chance of fabricating facts. |\n",
      "| **Learning from usage** | Every produced answer becomes data that can be analyzed to adjust retrieval and generation strategies for better accuracy. |\n",
      "\n",
      "Thus, storing the generated outputs fuels a self‑improving loop: the model continually feeds its own answers back into the retrieval system, making subsequent responses richer, more reliable, and better aligned with the user’s history.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:     – Illustrate a scenario where self‑retrieval improves the accuracy of subsequent responses.\n",
      "Answer:  **Scenario: An AI coding tutor using Self‑Retrieval to refine its answers**\n",
      "\n",
      "---\n",
      "\n",
      "### 1. User asks for help\n",
      "\n",
      "**Query:** “I’m learning recursion in Python. Could you show me a simple factorial function?”\n",
      "\n",
      "**First Run (Standard RAG)**  \n",
      "- The *retriever* pulls a few relevant code snippets and tutorials from its corpus.  \n",
      "- The *generator* (GPT‑style model) composes a reply:\n",
      "\n",
      "```\n",
      "def factorial(n):\n",
      "    if n == 0:                     # base case\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)  # recursive step\n",
      "```\n",
      "\n",
      "This answer is correct at a glance, but it lacks important details (error handling, documentation string, and it uses the informal comment style).  \n",
      "The model **stores** this answer in its dedicated repository because Self RAG says the output becomes future‑retrieval data (Step 2 of Self RAG).\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Second user query – same concept, a bit deeper\n",
      "\n",
      "**Query:** “Can you improve this code? Add input validation, and explain in comments how tail recursion is avoided?”\n",
      "\n",
      "**Retrieval & Self‑Retrieval Phase**  \n",
      "- The *retriever* searches the corpus and pulls the same original materials *plus* the earlier answer stored in the Self‑Retrieval repo.  \n",
      "- By retrieving its own first reply, the system sees:\n",
      "\n",
      "  *“I used a plain recursion and a basic base case – it might not handle negative inputs.”*\n",
      "\n",
      "  and notices an omission: no docstring, no validation.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Self‑Correction Phase\n",
      "\n",
      "- **Feedback loop:** The generative component now *evaluates* the old answer against a set of quality criteria (accuracy, completeness). It flags missing pieces and cross‑references them with authoritative sources that define standard best practices for a factorial implementation.\n",
      "\n",
      "- **Revision:** The model creates an improved version by *refining* its own old answer:\n",
      "\n",
      "```\n",
      "def factorial(n: int) -> int:\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a non‑negative integer using recursion.\n",
      "    Raises:\n",
      "        ValueError: If `n` is negative.\n",
      "    Returns:\n",
      "        The integer factorial of `n`.\n",
      "    \"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Negative values are not allowed.\")\n",
      "    if n == 0:                          # Base case: 0! = 1\n",
      "        return 1\n",
      "    # Recursion step\n",
      "    return n * factorial(n - 1)\n",
      "```\n",
      "\n",
      "- The model *stores* this new, more comprehensive response as another entry in its Self‑Retrieval repository.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Subsequent query – a different but related problem\n",
      "\n",
      "**Query:** “Show me how to convert this factorial function to a tail‑recursive version that works in Python.”\n",
      "\n",
      "**How Self‑Retrieval Helps:**\n",
      "\n",
      "- The AI looks at the corpus, finds generic tail‑recursion patterns, and *also* finds the two previous factorial implementations it has written.\n",
      "\n",
      "- By aligning the **patterns** from the old answers with tail‑recursive patterns, the model *synthesizes* the correct tail‑recursive version:\n",
      "\n",
      "```\n",
      "def factorial_tr(n: int, acc: int = 1) -> int:\n",
      "    \"\"\"\n",
      "    Tail‑recursive factorial: acc holds the running product.\n",
      "    \"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Negative values are not allowed.\")\n",
      "    if n == 0:\n",
      "        return acc\n",
      "    return factorial_tr(n - 1, n * acc)\n",
      "```\n",
      "\n",
      "Now, this output **includes the same error‑handling and docstring** features that the model’s self‑retrieval loop flagged before. It is *more consistent* with the style the user has seen and avoids the mistake of omitting validation.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Where accuracy improves over time\n",
      "\n",
      "| Cycle | New Insight Learned from Self‑Retrieval |\n",
      "|-------|----------------------------------------|\n",
      "| 1 (Initial) | Code is correct but lacks docstrings & validation |\n",
      "| 2 | Recognized need for documentation, validation, comments |\n",
      "| 3 | Maintains improvements when re‑implementing with new techniques |\n",
      "\n",
      "**Outcome** – Each answer borrows from the model’s *own* prior outputs and corrects the previous gaps, delivering steadily refined, accurate, and style‑consistent code snippets without additional external input after the first set of references.\n",
      "\n",
      "---\n",
      "\n",
      "**Key Self‑RAG Steps Illustrated**\n",
      "\n",
      "1. **Initial Retrieval and Generation** – First factorial example.  \n",
      "2. **Self‑Storage** – That response is put into the self‑repository.  \n",
      "3. **Iterative Self‑Retrieval** – Later queries pull the stored examples as context.  \n",
      "4. **Self‑Improvement Loop** – Feedback identifies missing elements; the model adapts.  \n",
      "5. **Enhanced Output** – Final answers benefit from this continuous self‑refinement.\n",
      "\n",
      "This example shows how Self RAG can *bootstrap* its own learning: by repeatedly pulling from its previous answers, a system becomes self‑correcting, ensuring increasingly accurate responses in successive interactions.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I’m not sure what you’d like help with. Could you let me know what question or topic you’re interested in?\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  8. **For each RAG variant, identify one challenge or limitation that the architecture must overcome.**\n",
      "Answer:  **Challenges / Limitations for Each RAG Variant**\n",
      "\n",
      "| RAG Variant | Typical Challenge / Limitation |\n",
      "|-------------|---------------------------------|\n",
      "| **Standard RAG** | *Relevancy‑bias*: The retrieval step can pull in documents that are technically relevant but irrelevant to the exact nuance of the user’s query, which can lead to hallucinations in the subsequent generated reply. |\n",
      "| **Corrective RAG** | *Validation bottleneck*: The correction loop depends on access to a reliable, trusted dataset for comparison; in domains lacking authoritative references, the system may not know whether a generated answer is actually wrong and thus may fail to correct itself. |\n",
      "| **Speculative RAG** | *Ranking noise*: When multiple speculative outputs are generated, choosing the “best” one via a feedback model can be hard; scoring can be biased by spurious or incomplete information, so the selected answer may still be flawed. |\n",
      "| **Fusion RAG** | *Conflict resolution*: Integrated sources often contradict one another, and determining which fact or viewpoint is most credible requires sophisticated credibility metrics that are not yet universally available. |\n",
      "| **Agentic RAG** | *Uncontrolled autonomy*: The model decides which documents to pull and which actions to take. Without strict safeguards, it might pursue sub‑optimal or unintended goals, potentially mis‑retrieving data or generating unsafe content. |\n",
      "| **Self RAG** | *Cyclic noise amplification*: Storing and later re‑retrieving the model’s own outputs can reinforce earlier errors or biases. Unless rigorously monitored, the self‑referential loop can amplify inaccuracies over time. |\n",
      "\n",
      "These represent the main architectural hurdles that practitioners often face when implementing or scaling each variant.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I didn’t see a question in your message. Could you let me know what you’d like help with regarding Retrieval‑Augmented Generation (RAG)?\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  9. **Why might an organization opt for a Speculative or Fusion RAG variant rather than Standard RAG when working on a policy‑driven or regulatory compliance chatbot?**\n",
      "Answer:  Policy‑driven or regulatory compliance chatbots operate under two key pressures:  \n",
      "\n",
      "1. **High‑stakes correctness** – mistakes can lead to non‑compliance, fines, or reputational damage.  \n",
      "2. **Multiple authoritative perspectives** – the same rule can be interpreted differently or may change over time.\n",
      "\n",
      "Given the definitions in the provided content, organizations may select **Speculative RAG** or **Fusion RAG** instead of **Standard RAG** for the following reasons:\n",
      "\n",
      "| Variant | Why it’s attractive for regulatory/chatbot use | Key characteristics from the source |\n",
      "|---------|-----------------------------------------------|-------------------------------------|\n",
      "| **Speculative RAG** | *Handles ambiguity and uncertainty*: By generating several candidate responses (Step 2: Generative Speculation) and then scoring them (Step 3: Feedback and Ranking), the model can surface the most plausible or compliant answer even when policy language is fuzzy. | • Multiple speculative responses <br>• Feedback‑driven ranking of those responses <br>• Select highest‑scoring answer |\n",
      "| **Fusion RAG** | *Builds consensus from divergent documents*: The retrieval of diverse documents (Step 1) and integration (Step 2) allow the bot to weigh multiple interpretations and resolve conflicts (Step 4). The final synthesized answer therefore reflects a balanced view that acknowledges differing viewpoints while prioritizing official guidelines. | • Diverse source collection <br>• Integration of common themes <br>• Conflict‑resolution logic in output generation |\n",
      "\n",
      "**Contrast with Standard RAG**\n",
      "\n",
      "Standard RAG simply selects the top‑k documents and uses them to generate an answer (Steps 1–3). For policy compliance, that approach risks:\n",
      "\n",
      "- **Over‑reliance on the “most relevant” single doc**, possibly overlooking important secondary sources or updates.  \n",
      "- **Lack of built‑in mechanism to handle contradictions** or to test whether a generated answer satisfies multiple authoritative references.  \n",
      "\n",
      "Speculative and Fusion variants therefore provide two layers that directly address these pain points:\n",
      "\n",
      "- **Speculative**: Redundant output paths guard against accidental omission or misinterpretation, enabling the system to pick the version that most consistently aligns with verified data.  \n",
      "- **Fusion**: Consolidates evidence from many viewpoints and incorporates an explicit conflict‑resolution step, producing a single, more robust, policy‑aligned response.\n",
      "\n",
      "In short, when regulatory compliance or policy accuracy is mission‑critical, selecting a RAG variant that deliberately probes for consistency across sources—by generating multiple plausible answers or by synthesising a cohesive view—helps the organization move beyond the single‑retrieval bias of Standard RAG and build confidence that the chatbot’s replies truly meet required standards.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I didn’t receive a question. Could you let me know what you’d like help with?\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  10. **In the context of a “Free Certification on RAGs” mentioned at the end of the document, what specific competencies or learning outcomes would you expect participants to attain?**  \n",
      "Answer:  A **Free Certification on RAGs** (Retrieval‑Augmented Generation) is likely to be aimed at preparing participants to design, evaluate and extend a complete RAG pipeline.  While the exact curriculum will vary with the issuer, a robust certification would generally cover the following competencies or learning outcomes:\n",
      "\n",
      "| Competency | What it means | Typical assessment / learning outcome |\n",
      "|-----------|--------------|----------------------------------------|\n",
      "| **Foundations of Retrieval & Generation** | Describe how a retrieval model (vector search, semantic similarity, dense retrievers, etc.) and a generative model (GPT‑style transformer) interact. | Ability to explain a “retrieval → generate → present” workflow in plain words. |\n",
      "| **Differentiating RAG Architectures** | Distinguish between the major variations shown in the notes – Standard, Corrective, Speculative, Fusion, Agentic, Self – including when each is appropriate. | Short answer questions that compare and contrast two architectures, and design‑choice scenarios. |\n",
      "| **Retrieval Design & Evaluation** | Choose encoding and similarity methods (e.g. SBERT, FAISS, HNSW), construct indexes, and evaluate recall/f‑measure, latency, and scaling considerations. | Lab activity: build an index for a sample corpus and report precision/recall metrics. |\n",
      "| **Generative Response Quality** | Use prompt engineering and finetuning to produce concise, accurate, context‑aware output that incorporates retrieved content. | Prompt‑design exercises with rubric scoring (relevance, coherence, factuality). |\n",
      "| **Correction & Validation** | Implement a validation loop that compares the generative output against trusted references and auto‑corrects errors (as in Corrective RAG). | Code a minimal validation step that flags fact‑checking mismatches. |\n",
      "| **Speculation & Selection** | Generate multiple hypotheses or responses (Speculative RAG), score them (confidence, relevance), and pick the best. | Assignment to build a simple multi‑output generator, run a scoring procedure, and choose top‑n. |\n",
      "| **Fusion & Conflict Resolution** | Combine signals from multiple retrieved sources, identify conflicts, weight them, and produce a single, consistent answer. | Case‑study: reconcile two contradictory facts and explain the chosen resolution rule. |\n",
      "| **Autonomous Planning** | Demonstrate Agentic RAG principles—set a high‑level goal, break it into sub‑retrieval‑plus‑generation steps, and iterate autonomously. | Capstone design: define a user query goal, outline an iterative plan and show outcome. |\n",
      "| **Self‑Refinement Cycles** | Store model‑produced outputs as pseudo‑data, retrieve from that self‑knowledge pool to improve future responses (Self RAG). | Documentation of a loop that ingests prior outputs, shows iterative improvement across epochs. |\n",
      "| **Ethics & Trustworthiness** | Identify hallucination risks, bias introduced via retrieval datasets, and ways to audit output. | Written reflection on a scenario where retrieval data could skew a generative answer. |\n",
      "| **Tooling & Deployment Basics** | Setup an end‑to‑end RAG pipeline in a popular framework (e.g. LangChain, Langsmith, Hugging Face pipelines). | Deploy a runnable prototype on a platform that accepts user questions. |\n",
      "| **Assessment & Continuous Learning** | Design metrics to monitor a live RAG system, and iterate on training data, index tuning, or prompting strategies. | Draft a monitoring plan for a live chatbot employing RAG. |\n",
      "\n",
      "### Putting It Together\n",
      "A well‑structured certification will blend theory with hands‑on tasks:\n",
      "\n",
      "1. **Conceptual quizzes** – ensuring students can identify and choose among RAG variants.  \n",
      "2. **Programming challenges** – building a retrieval index, coding a generation hook, or simulating a corrective loop.  \n",
      "3. **Evaluation projects** – generating a benchmark test suite and scoring answers on accuracy, relevance, and alignment to trusted sources.  \n",
      "4. **Discussion boards** or **peer reviews** – fostering deeper analysis of bias or hallucinations in practical examples.  \n",
      "\n",
      "By the time participants complete the course, they should confidently:\n",
      "\n",
      "* Explain the differences among the RAG paradigms,  \n",
      "* Build and test a functional RAG pipeline,  \n",
      "* Evaluate and improve retrieval accuracy and generative correctness, and  \n",
      "* Handle special scenarios (conflicting data, lack of evidence, self‑improvement loops) with transparent methods.  \n",
      "\n",
      "These outcomes directly mirror the varied content presented in the provided document and lay a solid groundwork for any future role that leverages Retrieval‑Augmented Generation.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:      – Which RAG techniques would most likely be emphasized?\n",
      "Answer:  The presentation places a **strong emphasis on the foundational approach – “Standard RAG,”** the classic two‑step workflow (retrieve, then generate). That is the go‑to methodology that guarantees relevance, traceability, and ease of debugging.\n",
      "\n",
      "Once the audience grasps Standard RAG, the training naturally moves to the *next‑level* refinement **“Corrective RAG.”**  Corrective RAG adds an additional validation/feedback loop, ensuring the answer is factually aligned with trusted sources.  Because this directly addresses hallucinations—a common pain point for practical deployments—most courses and certifications devote a significant portion to it.\n",
      "\n",
      "Less coverage is usually given to the “Speculative”, “Fusion”, “Agentic”, and “Self” RAG variants (they are advanced or niche, and the “free certification” on RAGs likely sticks to the reliable, production‑ready workflows). If these are mentioned they’re typically only as a “research‑plus / future‑of‑RAG” side note.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  It looks like you shared a bunch of notes on different RAG approaches. Is there something specific you’d like help with—perhaps a comparison, implementation advice, or clarification on any of these techniques? Let me know how I can assist!\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  *(These questions cover every element presented in the source – the definitions, step‑by‑step procedures, comparative benefits, practical scenarios, and the certification mention—ensuring none of the provided information is omitted.)*\n",
      "Answer:  **Comprehensive Overview of Retrieval‑Augmented Generation (RAG) Approaches**\n",
      "\n",
      "Below is a complete, point‑to‑point breakdown of every concept introduced in the source you supplied: the six main RAG variations, their step‑by‑step workflows, how they compare, realistic use cases, and even a reminder of the free RAG certification mentioned.  \n",
      "\n",
      "| # | RAG Type | Core Idea | Step‑by‑Step Pipeline | Typical Use‑Cases | Advantages/Key Insights |\n",
      "|---|---|---|---|---|---|\n",
      "| 1 | **Standard RAG** | Combine a retrieval engine with a generative model so that the generation is grounded in retrieved facts. | **1. Input** ➜ user query. **2. Retrieval** ➜ fetch most relevant docs/passages (vector or dense). **3. Rank** ➜ pick top‑k. **4. Generation** ➜ feed those passages into the large‑language model to generate a response. **5. Output** ➜ final answer shown to the user. | FAQs for a knowledge base, product help desks, chatbot replies, automated essay generation with evidence. | Good factual grounding, straightforward to implement; performance heavily relies on the retrieval step. |\n",
      "| 2 | **Corrective RAG** | Adds a validation/correction loop to ensure factual accuracy. | **1. Retrieve & generate** as in Standard. **2. Validation** ➜ compare content against trusted source(s). **3. Correct** ➜ adjust or regenerate if discrepancy found. **4. Iteration** ➜ repeat until output meets fidelity threshold. **5. Return** the validated response. | Legal compliance notes, medical summaries, scientific literature reviews where high‑accuracy is essential. | Minimizes hallucinations; costlier due to repeated passes, but essential where truth matters. |\n",
      "| 3 | **Speculative RAG** | Generates multiple hypothesis‑style answers, then selects the best via evaluation. | **1. Retrieve** → fetch relevant documents. **2. Generate Speculatively** → produce several distinct completions using the same context. **3. Evaluate/Rank** → score each via automatic metrics or supplementary retrieval. **4. Choose** → pick the highest‑scoring response. **5. Output**. | Creative writing, brainstorming ideas, disambiguation in ambiguous queries, product feature design suggestions. | Encourages diverse thinking; risk of picking a plausible yet wrong answer unless evaluation is robust. |\n",
      "| 4 | **Fusion RAG** | Synthesises evidence from multiple retrieved sources into a unified response. | **1. Retrieve** → gather a spread of documents covering various angles. **2. Integrate** → analyze content for themes, identify contradictions. **3. Synthesize** → craft single reply weaving together consistent data and noting differing perspectives. **4. Conflict‑Resolution** → use source credibility or rules to decide which info to follow. **5. Output**. | News aggregation, policy analysis, multi‑source reporting, academic literature surveys. | Delivers holistic insights, mitigates single‑source bias, handles conflicting evidence. |\n",
      "| 5 | **Agentic RAG** | AI acts autonomously toward a goal, iterating its own retrieval plan. | **1. Goal Definition** ➜ provide the mission (ex: “explore quantum computing potentials”). **2. Planning** ➜ the model internally drafts the needed queries / actions. **3. Retrieval ↔ Generation Loop** ➜ fetch, generate, reassess. **4. Adaptation** ➜ refine strategy based on intermediate feedback. **5. Goal Completion** ➜ finalize output. | Autonomous research assistants, strategy planning bots, creative story builders that need self‑driven structure. | Empowers self‑guided inquiry, dynamic strategy building; requires careful monitoring to avoid drift. |\n",
      "| 6 | **Self‑RAG** | Leverages the model’s prior outputs as new “data” for future queries—creating a self‑augmenting knowledge base. | **1. Standard RAG** ➜ produce response. **2. Store** ➜ archive generation as a new doc. **3. Retrieval** ➜ subsequent requests can search both external corpus and stored internal docs. **4. Learning Loop** ➜ patterns or gaps from stored data refine future retrieval choices. **5. Output** with continual self‑improvement. | Chatbots that remember past interactions, incremental documentation generation, building personal knowledge trees. | Allows continual fine‑tuning without external dataset updates; careful with accumulation of errors or drift. |\n",
      "\n",
      "### Comparative Snapshot\n",
      "\n",
      "| Feature | Standard | Corrective | Speculative | Fusion | Agentic | Self |\n",
      "|---|---|---|---|---|---|---|\n",
      "| **Primary Focus** | Fact‑grounding | Accuracy ↑ with validation | Diversity of outputs | Holistic integration | Autonomous action | Self‑learning |\n",
      "| **Key Trade‑Off** | Fastest, less robust | Slower, extra step | Possible hallucination if eval fails | Needs careful conflict resolution | Requires clear goals and monitoring | Needs safety for self‑accumulated noise |\n",
      "| **Best For** | Chat, FAQs | Domains needing high‑trust | Creative generation, uncertainty | Multi‑source overviews | Complex goal‑oriented tasks | Knowledge retention, personalization |\n",
      "\n",
      "### Practical Scenarios & When to Pick Which\n",
      "\n",
      "| Scenario | Ideal RAG | Why it works |\n",
      "|---|---|---|\n",
      "| Customer‑service bot that needs accurate policy info | Corrective | Checks facts against authoritative policy docs. |\n",
      "| A travel planner suggesting itineraries with multiple viewpoints (budget, adventure, history) | Fusion | Merges travelers’ preferences from multiple itineraries and balances. |\n",
      "| A brainstorming aid for inventors seeking new ideas | Speculative | Generates diverse proposals for later narrowing down. |\n",
      "| An autonomous research assistant writing a literature review | Agentic | Autonomously charts through research questions and iterates searches. |\n",
      "| A chat system that gradually builds knowledge about the user | Self | Stashes past chats for future recall and enrichment. |\n",
      "| A basic knowledge‑base question answerer | Standard | Straightforward fact‑driven answers. |\n",
      "\n",
      "### Certification Mention\n",
      "\n",
      "The source concludes by promoting a **free certification on RAGs** hosted by Harshit Ahluwalia. Check the “link in the description” (as referred to in the source) for details and enrolment.\n",
      "\n",
      "---\n",
      "\n",
      "**Summary:**  \n",
      "The six RAG variants form a spectrum of strategies for augmenting language models with external knowledge: from straightforward factual grounding (Standard) to sophisticated self‑learning loops (Self). The right choice hinges on whether the priority is factual integrity, diverse hypotheses, comprehensive synthesis, autonomous problem solving, or incremental self‑education.  \n",
      "\n",
      "If you’d like to explore a particular type in depth, test it in a demo, or get guidance on implementing the free certification, just let me know!\n",
      "--------------------------------------------------\\n\\n\n"
     ]
    }
   ],
   "source": [
    "# Answer each question and save to a file\n",
    "for question in ques_list:\n",
    "    print(\"Question: \", question)\n",
    "    answer = answer_generation_chain.run(question)\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"--------------------------------------------------\\\\n\\\\n\")\n",
    "    # Save answer to file\n",
    "    with open(\"answers.txt\", \"a\") as f:\n",
    "        f.write(\"Question: \" + question + \"\\\\n\")\n",
    "        f.write(\"Answer: \" + answer + \"\\\\n\")\n",
    "        f.write(\"--------------------------------------------------\\\\n\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed305a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
